{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Notebook","text":"<p>\u8fd9\u662f BottleFish \u7684\u7b14\u8bb0\u672c\u3002\u4e3b\u8981\u8bb0\u5f55\u4e86\u4f5c\u8005\u5173\u4e8eAI\u3001CAD\u7b49\u8bba\u6587\u7684\u9605\u8bfb\u7b14\u8bb0\u3002</p>"},{"location":"AI/LLM/Arch/GPT1/","title":"GPT-1","text":"<p>Reference Paper</p> <p>Improving Language Understanding by Generative Pre-Training</p> <p>Author: OpenAI Date: 2018</p>"},{"location":"AI/LLM/Arch/GPT1/#1-framework","title":"1 Framework","text":"<p>Our training procedure consists of two stages. The first stage is learning a high-capaciry language model on a large corpus of text. This is followed by a fine-tuning sage, where we adapt the model to a discriminative task with labeled data.</p>"},{"location":"AI/LLM/Arch/GPT1/#11-unsupervised-pre-training","title":"1.1 Unsupervised pre-training","text":"<p>Given an unsupervised corpus of tokens \\(\\mathcal{U}=\\{u_1,...,u_n\\}\\), we use a standard language modeling objective to maximize the following likelihood:</p> \\[ L_1(\\mathcal{U})=\\sum_i \\log P(u_i|u_{i-k},...,u_{i-1}; \\Theta) \\] <p>where \\(k\\) is the size of the context window, and the conditional probability \\(P\\) is modeled using a neural network with parameters \\(\\Theta\\). These parameters are trained using stochastic gradient descent.</p> <p>In GPT-1, we use a multi-layer Transformer decoder for the language model. This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens:</p> \\[ \\begin{aligned} h_0&amp;=UW_e+W_p \\\\ h_l&amp;=\\text{transformer\\_block}(h_{l-1}) \\quad \\forall i \\in [1,n] \\\\ P(u)&amp;=\\text{softmax}(h_nW_e^T) \\end{aligned} \\] <p>where \\(U=(u_{-k},...,u_{-1})\\) is the context vector of tokens, \\(n\\) is the number of layers, \\(W_e\\) is the token embedding matrix, and \\(W_p\\) is the position embedding matrix.</p>"},{"location":"AI/LLM/Arch/GPT1/#12-supervised-fine-tuning","title":"1.2 Supervised fine-tuning","text":"<p>We assume a labeled dataset \\(\\mathcal{C}\\), where each instance consists of a sequence of input tokens, \\(x^1,...,x^m\\), along with a label \\(y\\). The inputs are passed through our pre-trained model to obtain the final transformer block's activation \\(h_l^m\\), which is then fed into an added linear output layer with parameters \\(W_y\\) to predict \\(y\\):</p> \\[ P(y|x^1,...,x^m)=\\text{softmax}(h_l^mW_y) \\] <p>This gives us the following objective to maximize:</p> \\[ L_2(\\mathcal{C})=\\sum_{(x,y)}\\log P(y|x^1,...,x^m) \\] <p>We additionally found that including language modeling as an auxiliary objective to the fine-tuning helped learning by:</p> <ul> <li>improving generalization of the supervised model</li> <li>accelerating convergence</li> </ul> <p>Specifically, we optimize the following objective(with weight \\(\\lambda\\))</p> \\[ L_3(\\mathcal{C})=L_2(\\mathcal{C})+\\lambda * L_1(\\mathcal{C}) \\]"},{"location":"AI/LLM/Arch/GPT1/#13-task-specific-input-transformations","title":"1.3 Task-specific input transformations","text":"<p>For some tasks, like classification, we can directly fine-tune our model as described above.</p> <p>Certain other tasks, like question answering or textual entailment, have structured inputs such as ordered sentence pairs, or triplets of document, question, and answers. Since our pre-trained model was trained on contiguous sequences of text, we require some modifications to apply it to these tasks.</p> <p>We use a traversal-style approach, where we convert structured inputs into an ordered sequence that our pre-trained architecture across tasks. All transformations include adding randomly initialized start and end tokens \\((\\langle s \\rangle,\\langle e \\rangle)\\).</p> <ol> <li> <p>Textual entailment     For entailment tasks, we concatenate the premise \\(p\\) and hypothesis \\(h\\) token sequences, with a delimiter token ($) in between.</p> </li> <li> <p>Similarity     For similarity tasks, there is no inherent ordering of the two sentences being compared. To reflect this, we modify the input sequence to contain both possible sentence orderings(with a delimiter in between) and process each independently to produce two sequence representations \\(h_l^m\\) which are added element-wise before being fed into the linear output layer.</p> </li> <li> <p>Question Answering and Commonsense Reasoning     For these tasks, we are given a vontext document \\(z\\), a question \\(q\\), and a set of possible answers \\(\\{a_k\\}\\). We concatenate the document context and question with each possible answer, adding a delimiter token in between to get \\([z;q;\\$;a_k]\\). Each of these sequences are processed independently with ourmodel and then mormalized via a softmax layer to proudce an output distribution over possible answers.</p> </li> </ol>"},{"location":"AI/LLM/PostTrain/RL/","title":"DeepSeek-R1: Reinforcement Learning","text":"<p>Reference Paper</p> <p>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</p> <p>Author: OpenAI Date: 2018</p>"},{"location":"AI/LLM/Transformer/","title":"Transformer","text":"<p>Reference Paper</p> <p>Attention Is All You Need</p> <p>Author: Google Date: 2 Jun 2017</p> <p>Abstract</p> <p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</p>"},{"location":"AI/LLM/Transformer/#1-model-architecture","title":"1 Model Architecture","text":"<p>Transformer follows the encoder-decoder architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.</p> <pre><code>class EncoderDecoder(nn.Module):\n    \"\"\"\n    A standard Encoder-Decoder architecture. Base for this and many\n    other models.\n    \"\"\"\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        \"Take in and process masked src and target sequences.\"\n        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n\n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n\nclass Generator(nn.Module):\n    \"Define standard linear + softmax generation step.\"\n\n    def __init__(self, d_model, vocab):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n\n    def forward(self, x):\n        return log_softmax(self.proj(x), dim=-1)\n</code></pre>"},{"location":"AI/LLM/Transformer/#11-encoder-and-decoder","title":"1.1 Encoder and Decoder","text":"<p>Encoder: The encoder is composed of a stack of \\(N=6\\) identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.</p> <pre><code>def clones(module, N):\n    \"Produce N identical layers.\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\nclass Encoder(nn.Module):\n    \"Core encoder is a stack of N layers\"\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, mask):\n        \"Pass the input (and mask) through each layer in turn.\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n</code></pre> <p>We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is \\(\\text{LayerNorm}(x+\\text{Sublayer}(x))\\), where \\(\\text{Sublayer}(x)\\) is the function implemented by the sub-layer itself.</p> <pre><code>class LayerNorm(nn.Module):\n    \"Construct a layernorm module (See citation for details).\"\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\nclass SublayerConnection(nn.Module):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.\n    \"\"\"\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\n</code></pre> <p>Note</p> <p>Here, in the code implementation, we use pre-LN instead of normal-LN, that is, our implementation is </p> \\[ \\text{out}=x+\\text{Dropout}(\\text{Sublayer}(\\text{LayerNorm(x)})) \\] <p>We use Pre-LN because it stabilizes gradient flow, improves convergence, and makes it possible to train very deep Transformer models reliably.</p> <pre><code>class EncoderLayer(nn.Module):\n    \"Encoder is made up of self-attn and feed forward (defined below)\"\n\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n\n    def forward(self, x, mask):\n        \"Follow Figure 1 (left) for connections.\"\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n</code></pre> <p>Decoder: The decoder is also composed of a stack of \\(N=6\\) identical layers.</p> <pre><code>class Decoder(nn.Module):\n    \"Generic N layer decoder with masking.\"\n\n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)\n</code></pre> <p>In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.</p> <pre><code>class DecoderLayer(nn.Module):\n    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        \"Follow Figure 1 (right) for connections.\"\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n</code></pre> <p>We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position \\(i\\) can depend only on the known outputs as positions less than \\(i\\).</p>"},{"location":"AI/LLM/Transformer/#12-attention","title":"1.2 Attention","text":"<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>"},{"location":"AI/LLM/Transformer/#121-scaled-dot-product-attention","title":"1.2.1 Scaled Dot-Product Attention","text":"<p>The input consists of queries and keys of dimension \\(d_k\\), and values of dimension \\(d_v\\). We compute the dot products of the query with all keys, divide each by \\(\\sqrt{d_k}\\), and apply a softmax function to obtain the weights on the values.</p> <p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix \\(Q\\). The keys and values are also packed together into matrices \\(K\\) and \\(V\\). We compute the matrix of output as:</p> \\[ \\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V \\] <p>While for small values of \\(d_k\\), additive attention and dot-product(multiplicative) attention perform similarly, additive attention outperforms dot product attention without scaling for large values of \\(d_k\\). We suspect that for large values of \\(d_k\\), the dot products grow large in magnitude, pushing the softmax function into regions where it has extermely small gradients. To counteract this effect, we scale the dot products by \\(1/\\sqrt{d_k}\\).</p> <pre><code>def attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = scores.softmax(dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n</code></pre>"},{"location":"AI/LLM/Transformer/#122-multi-head-attention","title":"1.2.2 Multi-Head Attention","text":"<p>Instead of performing a single attention function with \\(d_{model}\\)-dimensional keys, values and queries, we found it benefical to linearly project the queries, keys and values \\(h\\) times with different, learned linear projections to \\(d_k\\), \\(d_k\\) and \\(d_v\\) dimensions, respectively.</p> <p>On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding \\(d_v\\)-dimensional output values. These are concatenated and once again projected, resulting in the final values.</p> <p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p> \\[ \\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O \\] <p>where \\(\\text{head}_i=\\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\\)</p> <p>where the projections are parameter matrices \\(W_i^Q \\in \\mathbb{R}^{d_model \\times d_k}\\), \\(W_i^K \\in \\mathbb{R}^{d_model \\times d_k}\\), \\(W_i^V \\in \\mathbb{R}^{d_model \\times d_v}\\) and \\(W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}\\)</p> <pre><code>class MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        \"Take in model size and number of heads.\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        \"Implements Figure 2\"\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model =&gt; h x d_k\n        query, key, value = [\n            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n            for lin, x in zip(self.linears, (query, key, value))\n        ]\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, self.attn = attention(\n            query, key, value, mask=mask, dropout=self.dropout\n        )\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = (\n            x.transpose(1, 2)\n            .contiguous()\n            .view(nbatches, -1, self.h * self.d_k)\n        )\n        del query\n        del key\n        del value\n        return self.linears[-1](x)\n</code></pre>"},{"location":"AI/LLM/Transformer/#123-applications-of-attention-in-transformer","title":"1.2.3 Applications of Attention in Transformer","text":"<p>The Transformer uses multi-head attention in three different ways:</p> <ul> <li>In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.</li> <li>The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.</li> <li>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regerssive property. We implement this inside of scaled dot-product attention by masking out(setting to \\(-\\infty\\)) all values in the input of the softmax which correspond to illegal connections.</li> </ul>"},{"location":"AI/LLM/Transformer/#13-position-wise-feed-forward-networks","title":"1.3 Position-wise Feed-Forward Networks","text":"<p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.</p> \\[ \\text{FFN}(x) = \\max(0,xW_1+b_1)W_2+b_2 \\] <p>While the linear transformations are the same across different positions, they use different parameters from layer to layer.</p> <pre><code>class PositionwiseFeedForward(nn.Module):\n    \"Implements FFN equation.\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(self.w_1(x).relu()))\n</code></pre>"},{"location":"AI/LLM/Transformer/#14-embeddings-and-softmax","title":"1.4 Embeddings and Softmax","text":"<p>We use learned embeddings to convert the input tokens and output tokens to vectors of dimension \\(d_model\\). We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and pre-softmax linear transformation. In the embedding layers, we multiply those weights by \\(\\sqrt{d_model}\\)</p> <pre><code>class Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.lut(x) * math.sqrt(self.d_model)\n</code></pre>"},{"location":"AI/LLM/Transformer/#15-positional-encoding","title":"1.5 Positional Encoding","text":"<p>In order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.</p> <p>In this work, we use sine and cosine functions of different frequencies:</p> \\[ \\begin{aligned} PE_{(pos,2i)} &amp;= \\sin(pos/10000^{2i/d_model}) \\\\ PE_{(pos,2i+1)} &amp;= \\cos(pos/10000^{2i/d_model}) \\end{aligned} \\] <p>where \\(pos\\) is the position and \\(i\\) is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid.</p> <pre><code>class PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n        return self.dropout(x)\n</code></pre>"},{"location":"AI/LLM/Transformer/#2-full-model","title":"2 Full Model","text":"<p>Here we define a function from hyperparamters to a full model</p> <pre><code>def make_model(\n    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n):\n    \"Helper: Construct a model from hyperparameters.\"\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model)\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n    model = EncoderDecoder(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n        Generator(d_model, tgt_vocab),\n    )\n\n    # This was important from their code.\n    # Initialize parameters with Glorot / fan_avg.\n    for p in model.parameters():\n        if p.dim() &gt; 1:\n            nn.init.xavier_uniform_(p)\n    return model\n</code></pre>"},{"location":"AI/LLM/Transformer/#3-training","title":"3 Training","text":""},{"location":"AI/LLM/Transformer/#31-optimizer","title":"3.1 Optimizer","text":"<p>We use the Adam optimizer with \\(\\beta_1=0.9\\), \\(\\beta_2=0.98\\) and \\(\\epsilon=10^{-9}\\). We varied the learning rate over the course of training, according to the formula:</p> \\[ lrate=d_{model}^{-0.5} \\cdot \\min(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5}) \\] <p>This corresponds to increasing the learning rate linearly for the first \\(warmup\\_steps\\) training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.</p>"},{"location":"AI/LLM/Transformer/#32-regularization","title":"3.2 Regularization","text":"<p>Residual Dropout We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.</p> <p>Label Smoothing During training, we employed label smoothing of value \\(\\epsilon_{ls}=0.1\\). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p>"},{"location":"AI/TSFM/Kronos/","title":"Kronos: A Foundation Model for the Language of Financial Markets","text":"<p>Reference Paper</p> <p>Kronos: A Foundation Model for the Language of Financial Markets</p> <p>Author: Department of Automation Tsinghua University</p> <p>Date: 2 Aug 2025</p> <p>Abstract</p> <p>The success of large-scale pre-training paradigm, exemplified by Large Language Models (LLMs), has inspired the development of Time Series Foundation Models (TSFMs). However, their application to financial candlestick (K-line) data remains limited, often underperforming non-pre-trained architectures. Moreover, existing TSFMs often overlook crucial downstream tasks such as volatility prediction and synthetic data generation. To address these limitations, we propose Kronos, a unified, scalable pre-training framework tailored to financial K-line modeling. Kronos introduces a specialized tokenizer that discretizes continuous market information into token sequences, preserving both price dynamics and trade activity patterns. We pre-train Kronos using an autoregressive objective on a massive, multi-market corpus of over 12 billion K-line records from 45 global exchanges, enabling it to learn nuanced temporal and cross-asset representations. Kronos excels in a zero-shot setting across a diverse set of financial tasks. On benchmark datasets, Kronos boosts price series forecasting RankIC by 93% over the leading TSFM and 87% over the best non-pre-trained baseline. It also achieves a 9% lower MAE in volatility forecasting and a 22% improvement in generative fidelity for synthetic K-line sequences. These results establish Kronos as a robust, versatile foundation model for end-to-end financial time series analysis. Our pre-trained model is publicly available at https://github.com/shiyu-coder/Kronos.</p>"},{"location":"AI/TSFM/Kronos/#1-introduction","title":"1. Introduction","text":"<p>Financial markets is a critical and challenging application area for TSFMs, given their inherent data richness, high-frequency observations, and complex, non-stationary temporal dynamics. At the core of this domain are K-line sequences, multivariate time series derived from candle-stick charts that record Open, High, Low, and Close prices, along with trading Volume and Amount(Turnover) over fixed intervals(OHLCVA). These sequences constitute a highly compact, information-dense \"language\" through which market participants interpret prices movements, volatility regimes, liquidity shifts, and colletive sentment.</p>"},{"location":"AI/TSFM/Kronos/#2-motivation","title":"2. Motivation","text":"<p>Despite the promise of TSFMs. Old application to finance faces significant hurdles.</p> <ol> <li>Unique statistical properties of K-line data<ul> <li>Low signal-to-noise ratio, strong non-stationarity, and complex interdependencies among OHLCVA features.</li> <li>These properties are poorly aligned with the inductive biases of general-purpose TSFMs.</li> </ul> </li> <li>Underserved financial domain in pre-training<ul> <li>Most TSFMs are trained on broad, multi-domain corpora where financial data accounts for only a tiny fraction.</li> <li>As a result, financial-specific dynamics are overlooked or averaged out.</li> </ul> </li> <li>Task gaps<ul> <li>Critical finance-specific tasks such as volatility prediction, synthetic data generation, and risk management remain largely unaddressed by existing TSFMs.</li> </ul> </li> </ol> <p>Empirical evidence shows that general-purpose TSFMs often underperform even non-pre-trained, specialized models (like iTransformer) on financial tasks. This reveals a gap: the field lacks a foundation model built specifically for financial time series.</p>"},{"location":"AI/TSFM/Kronos/#3-preliminary","title":"3. Preliminary","text":"<p>Let \\(D\\)-dimensional vector \\(x_t \\in \\mathbb{R}^D\\) denote the K-line observation at discrete time \\(t\\), comprising \\(D\\) key financial indicators. In this work, the dimension was fixed \\(D=6\\) to represent OHLCVA attributes. Given a historical sequence \\(x_{1:T}=(x_1,x_2,...,x_T)\\), our objective is to predict the following \\(H\\) observations \\(\\hat{x}_{T+1:T+H}=(\\hat{x}_{T+1}, \\hat{x}_{T+2},...,\\hat{x}_{T+H})\\).</p> <p>Rather than operating on raw continuous inputs, Kronos first quantizes each multivariate observation \\(x_t\\) into a discrete token \\(b_t\\) via a learnable codebook \\(\\mathcal{C}\\). Consequently, the original sequence \\(x_{1:T}=(x_1,x_2,...,x_T)\\) is mapped to \\(b_{1:T}=(b_1,...,b_T)\\). The forecasting task then reduces to an autoregressive token-sequence modeling problem:</p> \\[ p(\\mathbf{b}_{T+1:T+H}|\\mathbf{b}_{1:T}) = \\sum_{h=1}^{H} p(b_{T+h}|\\mathbf{b}_{1:T+h-1}) \\]"},{"location":"AI/TSFM/Kronos/#4-methodology","title":"4. Methodology","text":"<p>Kronos implements this via a two-phase framework: (1) K-line Tokenization and (2) Autoregressive Pre-training</p>"},{"location":"AI/TSFM/Kronos/#41-k-line-tokenization","title":"4.1 K-line Tokenization","text":"<p>The tokenization is achieved using a Transformer-based autoencoder composed of an encoder \\(E_{\\text{enc}}\\), a quantizer \\(Q\\), and a decoder \\(E_{\\text{dec}}\\).</p> <p>Kronos adapt Binary Spherical Quantization(BSQ) for this task. BSQ quantizes a continuous latent vector \\(\\xi_t\\) into a \\(k\\)-bit binary code \\(b_t \\in \\{-1,1\\}^k\\) by projecting it onto a set of learnable hyperplanes.</p>"},{"location":"AI/TSFM/Kronos/#411-introduction-to-bsq","title":"4.1.1 Introduction to BSQ","text":"<p>Related Paper</p> <p>Image and Video Tokenization with Binary Spherical Quantization</p> <p>Binary Spherical Quantization (BSQ) optimizes over an implicit codebook \\(\\textbf{C}_{BSQ}=\\{-\\frac{1}{\\sqrt{L}}, \\frac{1}{\\sqrt{L}}\\}^L\\), a hypercube projected onto a unit sphere. Each corner \\(\\textbf{c}_k \\in \\textbf{C}_{BSQ}\\) of a hypercube corresponds to a unique token \\(k\\). The quantizer works as follows: it projects some high-dimensional latent embedding \\(\\textbf{z}\\) to a lower-dimensional unit hypersphere \\(\\textbf{u}\\), applies binary quantization per axis \\(\\hat{\\textbf{u}}=sign(\\textbf{u})\\) and back-projects to the quantized vector in the original latent space \\(\\hat{\\textbf{x}}\\). Specifically, we start with an encoded visual input \\(\\textbf{z}=\\mathcal{E}(\\textbf{x}) \\in \\mathbb{R}^d\\). We first linearly project the latent embedding to \\(L\\) dimensions \\(\\textbf{v} = Linear(\\textbf{z}) \\in \\mathbb{R}^L\\), where \\(L \\ll d\\). Next, we obtain project \\(\\textbf{v}\\) onto the unit sphere \\(\\textbf{u}=\\frac{\\textbf{v}}{|\\textbf{v}|}\\) and perform binary quantization to each dimension of \\(u\\) independently \\(\\hat{\\textbf{u}}=\\frac{1}{\\sqrt{L}} sign(\\textbf{\\textbf{u}})\\), where \\(sign(x)\\) is the sign function. To keep outputs on the unit sphere, we map \\(sign(0) \\rightarrow 1\\). We use a Straight-Through Estimator (STE) make the operator differentiable, \\(sign_{STE}(x) = sg(sign(x)\u2212x) + x\\), where \\(sg(\u00b7)\\) denotes the stop-gradient operation. Finally, we back-project the quantized \\(\\hat{\\textbf{u}}\\) to the \\(d\\)-dimensional space \\(\\hat{\\textbf{z}} = Linear(\\hat{\\textbf{u}}) \\in \\mathbb{R}^d\\).</p>"},{"location":"AI/TSFM/Kronos/#412-effectiveness-of-bsq","title":"4.1.2 Effectiveness of BSQ","text":"<p>The effectiveness of BSQ tokenizer can be analyzed from two key perspectivs: its inherent noise suppression and its ability to create a structured, discrete state space suitable for sequence modeling.</p>"},{"location":"AI/TSFM/Kronos/#noise-suppression-and-stability","title":"Noise Suppression And Stability","text":"<p>BSQ's projection of embeddings onto a unit sphere prior to binarization guarantees that the expected distortion is strictly upper-bounded:</p> \\[ \\mathbb{E}_u\\|u-\\hat{u}\\| &lt; \\sqrt{2-\\frac{2}{\\sqrt{L}}} &lt; \\sqrt{2} \\] <p>This bound tightens as the codebook dimension \\(L\\) increases. In contrast, simple methods like sign-based quantization without normalization(e.g., LFQ) lack such a guarantee, leaving them vulnerable to arbitarily large errors from outlier inputs.</p>"},{"location":"AI/TSFM/Kronos/#learning-in-a-compact-and-discrete-state-space","title":"Learning in a Compact and Discrete State Space","text":"<p>High-frequency financial data exists in a high-dimensional, continuous state space. This tokenizer maps these in a finite, discrete vocabulary of tokens. This discretization serves as a powerful form of regularization with two main benefits.</p> <p>Inproved Sample Efficiency and Generalization: Instead of learning a complex function over a continuous space, a downstream model like a Transformer learns to predict transitions and patterns among a finite set of abstract states(tokens). This simplifies the learning tasks and also allows the model to learn robust patterns from fewer examples, which is particularly critical for modeling rare market phenomena.</p> <p>Reduced Overfitting: The quantization process inherently discards fine-grained, potentially noisy variations within each quantization cells. This prevents the model from fitting to spurious artifacts in the tarining data.</p>"},{"location":"AI/TSFM/Kronos/#hyperspherical-geometry-for-tail-sensitivity","title":"Hyperspherical geometry for tail sensitivity","text":"<p>In financial contexts, market returns and price changes often exhibit heavy tails(or fat tails). The heavy-tail distribution of price changes is one of the key sources of trading profits in quantitative investment and cannot be ignored.</p> <p>Unlike standard vector-quantization on the Euclidean sphere, BSQ's binary encoding preserves angular information very efficiently, making it more sensitive to fat-tail data that manifest as sharp directional changes in feature space. This aligns well how microstructure events often appear as abrupt shifts in the \"direction\" of the joint price-volume vector.</p>"},{"location":"AI/TSFM/Kronos/#413-subtoken-factorization","title":"4.1.3 Subtoken Factorization","text":"<p>While a large number of bits \\(k\\) (e.g., \\(k=20\\)) is desirable for capturing rich financial patterns, it results in an exponentially large vocabulary of size \\(2^k\\). To mitigate this, we use the subtoken factorization and factorize the \\(k\\)-bit code into \\(n\\) subspaces. Motivated by the trade-off between parameter savings and latency costs, we set \\(n=2\\). We partition the code into a coarse subtoken \\(b_t^c\\) and a fine subtoken \\(b_t^f\\) of equal bit length. The resulting code \\(b_t\\) is a concatenation of these two subtokens: \\(b_t=[b_t^c,b_t^f]\\).</p> <p>We train the tokenizer with a composite objective that combines a hierachical reconstruction loss and a commitment loss for BSQ:</p> \\[ \\mathcal{L}_{\\text{tokenizer}} = \\mathcal{L}_{\\text{coarse}} + \\mathcal{L}_{\\text{fine}} + \\lambda \\mathcal{L}_{\\text{quant}} \\] <p>where \\(\\lambda\\) is a balancing hyperparameter. The components are defined as:</p> <ul> <li>\\(\\mathcal{L}_{\\text{coarse}}=\\mathbb{E}[\\|\\textbf{x}-E_{\\text{dec}}(\\textbf{b}^c)\\|^2]\\), which trains the coarse subtoken \\(\\textbf{b}^c\\) to form a low-fidelity reconstruction.</li> <li>\\(\\mathcal{L}_{\\text{fine}}=\\mathbb{E}[\\|\\textbf{x}-E_{\\text{dec}}(\\textbf{b})\\|^2]\\), which evaluates the high-fidelity reconstruction using the complete token \\(\\textbf{b}\\).</li> <li>\\(\\mathcal{L}_{\\text{quant}}\\) is the quantization loss from BSQ that regularizes the learning process. It penalizes the L2 distance between continuous latent vector \\(\\xi\\) and their binary codes \\(\\textbf{b}\\), aligning the encoder's outputs with the learned codebook to ensure stable training.</li> </ul>"},{"location":"AI/TSFM/Kronos/#42-hierarchical-autoregressive-modeling","title":"4.2 Hierarchical Autoregressive Modeling","text":"<p>The primary objective is to estimate the joint distribution over the token sequence \\(\\textbf{b}=\\{b_1,...,b_r\\}\\). A simplified form of Equation 1 can be derived as:</p> \\[ p(\\textbf{b}) = \\sum_{t=1}^Tp(b_t|\\textbf{b}_{&lt;t}) \\] <p>where \\(\\textbf{b}_{&lt;t}\\) denotes all preceding tokens up to time \\(t-1\\).</p> <p>Given the hierarchical token design, in which each token is structured as \\(b_t=[b_t^c,b_t^f]\\), we further decompose the conditional probability using the chain rule to explicitly capture the inherent coarse-to-fine dependency:</p> \\[ p(b_t|\\textbf{b}_{&lt;t})=p(b_t^c|\\textbf{b}_{&lt;t}) \\cdot p(b_t^f|\\textbf{b}_{&lt;t}, b_t^c) \\] <p>At time \\(i\\), the subtokens \\(b_i^c\\) and \\(b_i^f\\) are independently projected into vector representations using two distinct embedding layers, resulting in representations \\(e_c(b_i^c)\\) and \\(e_f(b_i^f)\\), respectively. These embeddings are then concatenated and linearly projected to produce a fused input vector:</p> \\[ \\text{v}_i = W_{fuse}([e_c(b_i^c); e_f(b_i^f)]) \\] <p>where \\([\\cdot ; \\cdot]\\) denotes concatenation, and \\(W_{fuse}\\) is a learnable weight matrix responsible for projecting the combined representaion into the model's latent space.</p>"},{"location":"AI/TSFM/Kronos/#coarse-subtoken-prediction","title":"Coarse Subtoken Prediction","text":"<p>The history vector \\(\\textbf{h}_t\\) is projected by a linear head \\(W_c\\) to produce logits for the first subtoken's distribution:</p> \\[ p(b_t^c|\\textbf{b}_{&lt;c})=\\text{softmax}(W_c\\textbf{h}_t) \\]"},{"location":"AI/TSFM/Kronos/#fine-subtoken-prediction","title":"Fine Subtoken Prediction","text":"<p>During the trainiing, we use the model's own prediction from the previous step, \\(\\hat{b}_t^c\\), which is sampled from the predicted distribution \\(p(b_t^c|\\textbf{b}_{&lt;t})\\), rather than using the ground-truth subtoken(i.e., teacher-forcing). We use a cross-attention mechanism where the embedding of \\(\\hat{b}_t^c\\) acts as the query, and the history \\(\\textbf{h}_t\\) provides the key and value. The result is projected by the second head \\(W_f\\):</p> \\[ \\textbf{h}_t^{\\text{update}} = \\text{CrossAttn}(q=e_c(\\hat{b}_t^c), k=v=\\textbf{h}_t) \\] \\[ p(b_t^f|\\textbf{b}_{&lt;t},b_t^c)=\\text{softmax}(W_f\\textbf{h}_t^{\\text{update}}) \\] <p>The overall training objective \\(\\mathcal{L}_{ar}\\) is the negative log-likelihood of the data, summed over both prediction steps:</p> \\[ \\mathcal{L}_{\\text{ar}}=-\\mathbb{E}_{\\textbf{b}\\sim\\mathcal{D}} \\sum_{t=1}^T\\left[\\log p(b_t^c|\\textbf{b}_{&lt;t})+\\log p(b_t^f|\\textbf{b}_{&lt;t}, b_t^c)\\right] \\] <p>where \\(\\mathcal{D}\\) represents the data distribution.</p>"},{"location":"AI/TSFM/Kronos/#5-results-and-conclusion","title":"5. Results and Conclusion","text":""},{"location":"boolean/AdaptiveGridGeneration/","title":"Adaptive grid generation for discretizing impliit complexes","text":"<p>Quote</p> <p>Adaptive grid generation for discretizing implicit complexes</p> <p>date: 19 Jul 2024</p>"},{"location":"boolean/AdaptiveGridGeneration/#1-introduction","title":"1 Introduction","text":"<p>Implicit representations are widely used in computer graphis. A smooth and manifold surface in 3D can be represented as the level set of a scalar function \\(f:\\mathbb{R}^3 \\rightarrow \\mathbb{R}\\), also known as an implicit surface. The implicit surface enjoys a number of benefits, including a simple definition, easy modification, and convenience for operations such as offsets and boolean.</p> <p>To be useful for downstream applications, an implicit surface or complex must be discretized. An ideal grid for discretization should be adaptive in that the finer grid cells are typically where the implicit shape has a non-trival geometry or topology.</p> <p>While adaptive grid generatioin has been extensively studied for polygonizing implicit surfaces, works on implicit complexes have been scarce. A key challenge in the latter is adapting the grid structure to accuartely discretize the intersection of multiple implicit surface.</p>"},{"location":"boolean/AdaptiveGridGeneration/#2-preliminaries","title":"2 Preliminaries","text":""},{"location":"boolean/AdaptiveGridGeneration/#21-level-sets-and-zero-sets","title":"2.1 Level sets and zero sets","text":"<p>The Level set of a scalar function \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) at level \\(s \\in \\mathbb{R}\\) which we denote by \\(f^{-1}(s)\\), is the loci of points where \\(f\\) evaluates to \\(s\\):</p> \\[ f^{-1}(s) = \\{x\\in \\mathbb{R}^n | f(x)=s\\} \\] <p>We call the level set \\(f^{-1}(0)\\) the zero set, which we abbreviate as \\(f^{-1}\\).</p> <p>This definition naturally generalizes to the level set of a vector-valued function \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\), where \\(m &gt; 1\\), as the loci of points where \\(f\\) evaluated to a vectro level \\(s\\). Note that \\(f\\) can be considered as a set of \\(m\\) scalar functions \\(\\{f_1,...,f_m\\}\\), which we called the components of \\(f\\). Geometrically, \\(f^{-1}(s)\\) is the intersection of the \\(m\\) scalar level sets of its components,</p> \\[ f^{-1}(s) = \\bigcap_{i=1}^{m}f_i^{-1}(s_i) \\] <p>where \\(s=\\{s_1,...,s_m\\}\\). As a result, \\(f^{-1}(s)\\) is a \\((n-m)\\)-dimensional manifold, and it generally only exists for \\(m \\le n\\).</p>"},{"location":"boolean/AdaptiveGridGeneration/#3-refinement-criteria-for-zero-set","title":"3. Refinement criteria for zero set","text":"<p>Our criteria answer the following general question: given a function \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) for any dimension \\(n\\) and \\(m \\in [1,n]\\) and an \\(n\\)-simplex \\(t\\), does \\(t\\) need to be refined to better discretize the zero set \\(f^{-1}\\).</p> <p>Sepcial Case</p> <p>In \\(\\mathbb{R}^3\\), our criteria consider a single implicit surface(\\(m=1\\)), the intersection curve of two surfaces(\\(m=2\\)), or the interserction point of three surfaces(\\(m=3\\)).</p> <p>Our criteria follow the same principles as existing ones for implicit surfaces. That is, refinement is not needed if either \\(t\\) does not contain any part of \\(f^{-1}\\), or if \\(f^{-1}\\) is already close enough to the discretization. Specifically, let \\(\\bar{f}\\) be the linear approximation of \\(f\\) inside \\(t\\) by barycentric interpolation of values of \\(f\\) at the vertices of \\(t\\). We consider the zero set of \\(\\bar{f}\\), \\(\\bar{f}^{-1}\\), as the discretization of \\(f^{-1}\\) in \\(t\\). We deem \\(t\\) refinable for \\(f^{-1}\\) if it passes two tests:</p> <ul> <li> <p>Zero-crossing test:</p> \\[ f^{-1} \\cap t \\ne \\emptyset \\] </li> <li> <p>Distance test:</p> \\[ d_H(f^{-1} \\cap t, \\bar{f}^{-1}) &gt; \\epsilon \\] <p>where \\(d_H(X, Y) = \\text{sup}_{x\\in X}d(x,Y)\\) is the one-sided Hausdorff distance from \\(X\\) to \\(Y\\), and \\(\\pesilon\\) is a user-defined threshold.</p> </li> </ul>"},{"location":"boolean/AdaptiveGridGeneration/#31-proxy-construction","title":"3.1 Proxy construction","text":"<p>We consider the calss of functions \\(\\tilde{f}\\) represented as a convex combination with linear precision as our proxy. Such a function is defined by control points \\(p_1,...,p_l\\) that lie inside or on the boundary of the simplex \\(t\\), where each \\(p_i\\) is associated with a control value \\(b^i=\\{b_1^i,...,b_m^i\\}\\). The function, \\(\\tilde{f}\\), is a weighted average of \\(b^i\\),</p> \\[ \\tilde{f}(x)=\\sum_{i=1}^{l}w_i(x)b^i \\] <p>where the weights \\(w_i(x)\\) satisfy, for all \\(x \\in t\\):</p> <ol> <li>Convexity: \\(w_i(x) \\ge 0\\) for \\(i = 1,...,l\\), and \\(\\sum_{i=1}^lw_i(x)=1\\)</li> <li>Linear precision: \\(\\sum_{i=1}^lw_i(x)p_i=x\\)</li> </ol> <p>A example of such \\(\\tilde{f}\\) is the Bezier simplex, where the control points lie on a regular grid in \\(t\\) and the weights \\(w_i\\) are Berstein polynoials. In our implementation, we adopt the cubic Bezier simplex as the proxy. The control points \\(p_i\\) in a cubic Bezier simplex consist of all vertices of \\(t\\), the two trisector on each edge of \\(t\\), and the centroid of each triangle face of \\(t\\).</p>"},{"location":"boolean/AdaptiveGridGeneration/#32-zero-crossing-test","title":"3.2 Zero-crossing test","text":"<p>We now examine the refinement criteria on a proxy function \\(\\tilde{f}\\). The value \\(\\tilde{f}(x)\\) at any point \\(x \\in t\\) lies in the \\(m\\)-dimensional convex hull of \\(b\\). As a result, if there exists some points \\(x\\) such that \\(\\tilde{f}(x)=0\\), then \\(0\\) lies in this convex hull. So a necessary condition to pass the zero-crossing test is</p> \\[ O \\in CH(b) \\] <p>where \\(O\\) is the origin of \\(\\mathbb{R}^m\\) and \\(CH(b)\\) is the convex hull of \\(b\\).</p>"},{"location":"boolean/AdaptiveGridGeneration/#33-distance-test","title":"3.3 Distance test","text":"<p>We next turn to the distance test on the proxy \\(\\tilde{f}\\), which check if</p> \\[ d_H(\\tilde{f}^{-1}\\cap t, \\bar{f}^{-1}) &gt; \\epsilon \\] <p>where \\(\\bar{f}\\) is the linear approximation of \\(f\\) in \\(t\\). We observe that the exact Hausdorff distance has the following upper bound:</p> \\[ \\begin{aligned} d_H(\\tilde{f}^{-1} \\cap t, \\overline{f}^{-1})  &amp;= \\max_{x \\in \\tilde{f}^{-1}\\cap t} d(x, \\overline{f}^{-1}(0)) \\\\ &amp;= \\max_{x \\in \\tilde{f}^{-1}\\cap t} d(x, \\overline{f}^{-1}(\\tilde{f}(x))) \\\\ &amp;\\leq \\max_{x \\in t} d(x, \\overline{f}^{-1}(\\tilde{f}(x))) \\end{aligned} \\] <p>In other words, the upper bound is the maximal distance from any point \\(x \\in t\\) to the level set of the linear function \\(\\bar{f}\\) at the level \\(\\tilde{f}(x)\\).</p> <p>We first consider the simple case of \\(m=1\\). Since \\(\\bar{f}\\) is a scalar function, its level sets are \\((n-1)\\)-dimensional hyperplane in \\(\\mathbb{R}^n\\) orthogonal to the gradient \\(g=\\nabla \\bar{f}\\). The distance from \\(x\\) to the level set of \\(\\bar{f}\\) at level \\(\\tilde{f}(x)\\) is \\(\\frac{\\tilde{f}(x)-\\bar{f}(x)}{|g|}\\) (assuming \\(g\\ne 0\\)).</p> <p>Now consider \\(m &gt; 1\\). The vector level set \\(\\bar{f}^{-1}(\\tilde{f}(x))\\) is the intersection of \\(m\\) scalar level sets \\(\\bar{f}^{-1}_i(\\tilde{f}_i(x))\\), each being a hyperplane orthogonal to a gradient vector \\(g_i=\\nabla \\bar{f}_i\\). Let \\(v\\) be the vector from \\(x\\) to its nearest point on this intersection. We can therefore find \\(v\\) as</p> \\[ v = M(\\tilde{f}(x)-\\bar{f}(x)) \\] <p>where \\(M=g(g^Tg)^{-1}\\)</p> <p>Since linear transformations(such as \\(M\\)) preserve convexity, \\(v\\) lies in this convex hull of the transformed points \\(M(b-\\bar{b})\\). Finally, we arrive at the bound:</p> \\[ d(x, \\bar{f}^{-1}(\\tilde{f}(x))) = |v| = |M(\\tilde{f}(x)-\\bar{f}(x))| \\le \\max_{i=1,...,l}|M(b^i-\\bar{b}^i)| \\] <p>Therefore, a necessary condition to pass the distance test is</p> \\[ \\max_{i=1,...,l}|M(b^i-\\bar{b}^i)| &gt; \\epsilon \\]"},{"location":"boolean/AdaptiveGridGeneration/#4-simplicial-refinement","title":"4. Simplicial refinement","text":"<p>Our implementation is specialized for the practically useful cases of \\(n=2,3\\). Our method is a variation of the calssical longest edge bisection(LEB) method.</p> <p>For any chosen edge \\(e\\), we bisect \\(all\\) cells incident to \\(e\\) at the mid-point of \\(e\\), regardless of whether \\(e\\) is the longest edge of that cell. Specifically, we call an edge refinable if any of its incident cells is refinaable according to the given refinement criteria. At each iteration of the algorithm, we bisect the longest refinable edge in the current grid. We call our method the longest refinable edge bisection(LREB) method.</p> <p>Note</p> <p>This method lack a theoretical bound. But experimental evidence is provided to show that the worst quality among cells enclosing the implicit shape produced by LREB appears to be lower-bound for a variety of implicit surfaces and complexes.</p>"},{"location":"boolean/AdaptiveGridGeneration/#5-result","title":"5. Result","text":""},{"location":"boolean/AdaptiveGridGeneration/#51-zero-crossing-and-distance-test","title":"5.1 Zero-crossing and distance test","text":"<p>We first examine the tests on a scalar zero set whose implicit field is \\(f(x,y)=x^5+x^4-2y^2\\)</p> <p>We then examine the tests on vector zero sets. We start with a 2-component function \\(f=\\{f_1,f_2\\}\\) whose implicit curves are circles with large radius and are almost tangent at their intersection points.</p>"},{"location":"boolean/AdaptiveGridGeneration/#52-performance","title":"5.2 Performance","text":"<p>Observe that the running time is dominated by evaluating the function values and gradients at the grid points(red) and checking the criteria(blue), and the latter in turn is dominated by computing the proxy(solid line) and performing the tests on implicit surfaces(short dashes).</p>"},{"location":"boolean/FastAndRobustMeshArrangement/","title":"Fast and Robust Mesh Arrangements using Floating-point Arithmetic","text":"<p>Quote</p> <p>Fast and Robust Mesh Arrangements using Floating-point Arithmetic</p> <p>date: 27 Nov 2020</p> <p>Abstract</p> <p>We introduce a novel algorithm to transform any generic set of triangles in 3D space into a well-formed simplicial complex. Intersecting elements in the input are correctly identified, subdivided, and connected to arrange a valid configuration, leading to a topologically sound partition of the space into piece-wise linear cells. Our approach does not require the exact coordinates of intersection points to calculate the resulting complex. We represent any intersection point as an unevaluated combination of input vertices. We then extend the recently introduced concept of indirect predicates to define all the necessary geometric tests that, by construction, are both exact and effcient since they fully exploit the floating-point hardware. This design makes our method robust and guaranteed correct, while being virtually as fast as non-robust floating-point based implementations. Compared with existing robust methods, our algorithm o$ers a number of advantages: it is much faster, has a better memory layout, scales well on extremely challenging models, and allows fully exploiting modern multi-core hardware with a parallel implementation. We thoroughly tested our method on thousands of meshes, concluding that it consistently outperforms prior art. We also demonstrate its usefulness in various applications, such as computing effcient mesh booleans, Minkowski sums, and volume meshes.</p>"},{"location":"boolean/FastAndRobustMeshArrangement/#1-problem-statement","title":"1 Problem Statement","text":"<p>Consider a generic set of triangles \\(T\\) with no assumptions, and identify their arrangement in space, that is, a subdivision of the space in cells bounded by the input triangles. To explicitly represent the bounding surface of each cell we subdivide intersecting triangles, thus constructing new points to represent the intersections and connecting them to form the sub-triangles.</p> <p>We achieved this result by first cleaning \\(T\\) from degenerate(null area) elements, and then resolving all the intersections on the remaining triangles, thus producing a modified set \\(T'\\). Therefore, triangles in \\(T'\\) are not only geometrically coincident with \\(T\\) but also from a valid simplicial complex, meaning that they are either disjoint or connected through a shared sub-simplex(i.e., they have an edge or vertex in common).</p> <p>The most critical part of the computation of an arrangement is the representation of points of intersection.</p>"},{"location":"boolean/FastAndRobustMeshArrangement/#2-main-contribution","title":"2 Main Contribution","text":"<p>Main contribution is a novel method to robustly reconstruct the subdivided triangles \\(T'\\) without using the coordinates of intersection points. In the method, indeed, these points are implicitly represented in terms of the input geometic entities that generated them. Their relative positions, the program flow and the connectivity of \\(T'\\) are robustly determined by a novel set of exact geometric predicates.</p>"},{"location":"boolean/FastAndRobustMeshArrangement/#3-representation-and-processing","title":"3 Representation and Processing","text":"<p>Our implicit intersection points keep the memory of the supporting planes and lines of the triangles and edges that generated them, and the vertices of the supporting entities are part of the input, hence exact.</p> <p>Using an explicit representation, this is a relatively simple task to discover the relative position of all the intersection points for existing mesh elements, but for an implicit representation, it's not that obviously. So, we will implement these core functionalities for sets of implicit points, as well as for hybrid sets made by both implicit and explicit points later.</p>"},{"location":"boolean/FastAndRobustMeshArrangement/#31-point-representation","title":"3.1 Point representation","text":"<p>Our algorithm performs computations involving three different types of points, in terms of representation:</p> <ul> <li>explicit(i.e., input) points, for which exact floating-point coordinates are known a priori;</li> <li>point implicitly defined by the intersection of two triangles. We represent these points indirectly using five explicit points</li> <li>point implicitly defined by the intersection of three or more triangles. We represent these points indirectly with three triplets of explicit points, for a total of nine;</li> </ul> <p>Our key to implement these functionalities has been a set of novel geometric predicates that can opeate on any combination of point types.</p> <p>We create a multi-stage filtering to ensure that all the variables are(exact) input values, and the predicate can always return an exact sign:</p> <ol> <li>First, all the calculations are done in floating  point arithmetic. Along with the expression, a semi-static filter is also computed. If the magnitude of the evaluated expression is larger than the filter value, then its sign is guaranteed correct, and the process stops.</li> <li>If not, everything is recomputed using interval arithmetic. If the resulting interval does not contain the zero, the sign is correct and we stop.</li> <li>If not, we recompute everything using floating point expansions which always guarantee correctness.</li> </ol>"},{"location":"boolean/FastAndRobustMeshArrangement/#32-point-orientation","title":"3.2 Point orientation","text":"<p>A very basic geometric test: the computation of the relative position, on a plane, of a point and a line. This operator is called <code>orient2d</code>.</p> <p>Even though our points are embedded in \\(\\mathbb{R}^3\\), determining the orientation of a point with respect to a given line is intrinsically two-dimensional and requires expressing the point coordinates in a 2D frame. The rotatioin would require applying a rotation matrix which may introduce round-off errors. Absolute precision requires to drop a coordinate and therefore do not introduce imprecision in the other two coordinates.</p> <p>Let \\(t\\) have vertices \\(v_i\\), \\(v_j\\), \\(v_k\\), we consider the normal vector \\(\\vec{n} = (v_j - v_i) \\times (v_k-v_i)\\). When calculating the orientation of three points on \\(t\\), we just drop the coordinate associated to the biggest component between \\(|\\vec{n_x}|\\), \\(|\\vec{n_y}|\\), and \\(|\\vec{n_z}|\\). This guarantees that the triangle will not become degenerate under the orthogonal projection we defined.</p> <p>We implemented a semi-static filter to make sure that \\(n_i\\) is far enough from zero. This filter is \\(\\epsilon_n=8.88395 10^{-16}\\delta^2\\), where \\(\\delta\\) is the maximum magnitude among the nine coordinates of \\(v_i\\), \\(v_j\\) and \\(v_k\\). If \\(n_i\\) is larger than \\(\\epsilon_n\\), we can safely use it to proceed. Otherwise, we recalculate it exactly using expansion arithmetic.</p>"},{"location":"boolean/FastAndRobustMeshArrangement/#33-the-rest","title":"3.3 The rest","text":"<p>The remaining computations, whether the points are explicit or implicit, proceed identically. First, use the input point to represent implicit points and perturbation. Compair it with the filter, if it can not pass, recalculate it exactly using expansion arithmetic.</p>"},{"location":"boolean/interactiveMeshBoolean/","title":"Interactive and Robust Mesh Booleans","text":"<p>Reference Paper</p> <p>Interactive and Robust Mesh Booleans</p> <p>Date: 26 May 2022</p> <p>Abstract</p> <p>Boolean operations are among the most used paradigms to create and edit digital shapes. Despite being conceptually simple, the computation of mesh Booleans is notoriously challenging. Main issues come from numerical approximations that make the detection and processing of intersection points inconsistent and unreliable, exposing implementations based on floating point arithmetic to many kinds of degeneracy and failure. Numerical methods based on rational numbers or exact geometric predicates have the needed robustness guarantees, that are achieved at the cost of increased computation times that, as of today, has always restricted the use of robust mesh Booleans to offline applications. We introduce the first algorithm for Boolean operations with robustness guarantees that is capable of operating at interactive frame rates on meshes with up to 200K triangles. We evaluate our tool thoroughly, considering not only interactive applications but also batch processing of large collections of meshes, processing of huge meshes containing millions of elements and variadic Booleans of hundreds of shapes altogether. In all these experiments, we consistently outperform prior art by at least one order of magnitude.</p>"},{"location":"boolean/interactiveMeshBoolean/#introduction","title":"Introduction","text":"<p>In many existing methods, the calculation of a mesh Boolean is framed as a two step process. In the first step, conflicts between mesh elements are resolved, splitting triangles in order to incorporate intersection lines in the connectivity. In the second step, each mesh element is deemed as being inside or outside each input object. The result of a Boolean is eventually computed as a subset of the mesh elements generated in the first step, flitered according to the inside/outside labeling computed in the second step. As shown in the figure below.</p> <p>The major difficulty in implementing a Boolean pipeline comes from the use of finite precision arithmetic, which does not allow to exactly represent and test intersection points.</p>"},{"location":"mathematics/","title":"Mathematics","text":"<p>\u8fd9\u91cc\u8bb0\u5f55\u4e86\u6240\u6709\u6211\u6240\u4f7f\u7528\u548c\u9605\u8bfb\u8fc7\u7684\u5927\u90e8\u5206\u6570\u5b66\u5de5\u5177\u76f8\u5173\u7684\u8bba\u6587\u548c\u65b9\u6cd5\u3002</p>"},{"location":"mathematics/poissonEquation/","title":"Poisson's Equation","text":"<p>Related Works</p> <p>Stanford course Math 220B</p>"},{"location":"mathematics/poissonEquation/#1-problem-description","title":"1. Problem Description","text":"<p>We want to solve the Laplace's equation</p> \\[ \\Delta u = 0 \\] <p>and it's inhomogeneous versions, Poisson's equation</p> \\[ - \\Delta u = f \\] <p>We say a function u satisfying Laplace's equation is a harmonic function.</p>"},{"location":"mathematics/poissonEquation/#2-the-fundamental-solution","title":"2. The Fundamental Solution","text":"<p>Consider Laplace's equation in \\(\\mathbb{R}^n\\),</p> \\[ \\Delta u = 0 \\quad x \\in \\mathbb{R}^n \\] <p>Clearly, there are a lot of functions u which satisfy this equation. In particular, any constant function is harmonic. In addition, any function of the form \\(u(x)=a_1x_1 + ... + a_nx_n\\) for constants \\(a_i\\) is also a solution. Of course, we can list a number of others. Here, however, we are intrested in finding a particular solution of Laplace's equation which will allow us to solve Poisson's equation.</p> <p>Give the symmertic nature of Laplace's equation, we look for a radial solution. That is, we look for a harmonic funtion \\(u\\) on \\(\\mathbb{R}^n\\) such that \\(u(x)=v(|x|)\\). In addition, to being a natural choice due to the symmetry of Laplace's equation, radial solutions are natural to look for because they reduce a PDE to an ODE, which is generally easier to solve. Therefore, we look for a radial solution.</p> <p>If \\(u(x)=v(|x|)\\), then</p> \\[ u_{x_i} = \\frac{x_i}{|x|}v'(|x|) \\quad |x| \\neq 0 \\] <p>which implies</p> \\[ u_{x_ix_i} = \\frac{1}{|x|}v'(|x|) - \\frac{x_i^2}{|x|^3}v'(|x|) + \\frac{x_i^2}{|x|^2}v''(|x|) \\quad |x| \\neq 0 \\] <p>Therefore,</p> \\[ \\Delta u = \\frac{n-1}{|x|}v'(|x|) + v''(|x|) \\] <p>Letting \\(r = |x|\\), we see that \\(u(x)=v(|x|)\\) is a radial solution of Laplace's equation implies \\(v\\) satisfies</p> \\[ \\frac{n-1}{r}v'(r)+v''(r) = 0 \\] <p>And then, from some calculations, we see that for any constants \\(c1\\), \\(c2\\), the function</p> \\[ u(x) \\equiv \\begin{cases} c_1 \\ln |x| + c_2, &amp; \\text{if } n = 2 \\\\ \\frac{c_1}{(2 - n)|x|^{n - 2}} + c_2, &amp; \\text{if } n \\geq 3 \\end{cases} \\tag{3.1} \\] <p>for \\(x \\in \\mathbb{R}^n\\), \\(|x| \\neq 0\\) is a solution of Laplace's equation in \\(\\mathbb{R}^n - {0}\\). We notice that the function \\(u\\) defined in (3.1) satisfies \\(\\Delta u(x)=0\\) for \\(x \\neq 0\\), but at \\(x=0\\), \\(\\Delta u(0)\\) is undefined. We claim that we can choose constants \\(c_1\\) and \\(c_2\\) appropriately so that</p> \\[ - \\Delta_x u = \\delta_0 \\] <p>in the sense of distributions. Recall that \\(\\delta_0\\) is the distribution which is defined as follows. For all \\(\\phi \\in \\mathbb{D}\\),</p> \\[ (\\delta_0, \\phi) = \\phi(0) \\] <p>Claim 1</p> <p>For \\(\\Phi\\) defined in (3.3), \\(\\Phi\\) satisfies</p> \\[ -\\Delta_x\\Phi = \\delta_0 \\] <p>in the sense of distributions. That is, for all \\(g\\in \\mathcal{D}\\),</p> \\[ -\\int_{\\mathbb{R}^n}\\Phi(x)\\Delta_xg(x)\\,dx=g(0) \\] Proof <p>The proof of this claim is a little complex. You can reference to this handouts: Stanford course Math 220B laplace</p> <p>For now, though, assume we can prove this. That is, assume we can find constants \\(c_1\\),\\(c_2\\) such that \\(u\\) defined in (3.1) satisfied</p> \\[ - \\Delta_x u = \\delta_0 \\tag{3.2} \\] <p>Let \\(\\Phi\\) denote the solution of (3.2). Then, define</p> \\[ v(x) = \\int_{\\mathbb{R}^n} \\Phi(x - y) f(y) \\, dy \\] <p>\\(Formally\\), we compute the Laplacian of \\(v\\) as follows,</p> \\[ \\begin{aligned} - \\Delta_x v &amp;= - \\int_{\\mathbb{R}^n} \\Delta_x \\Phi(x - y) f(y) \\, dy \\\\              &amp;= - \\int_{\\mathbb{R}^n} \\Delta_y \\Phi(x - y) f(y) \\, dy \\\\              &amp;= \\int_{\\mathbb{R}^n} \\delta_x f(y) \\, dy = f(x) \\end{aligned} \\] <p>That is, \\(v\\) is a solution of Poisson's equation! Of course, this set of equalities above is entirely formal. We have not prove anything yet. However, we have motivated a solution formula for Poisson's equation from a solution to (3.2). We now return to using the radial solution (3.1) to find a solution of (3.2).</p> <p>Define the function \\(\\Phi\\) as follows. For \\(|x| \\neq 0\\), let</p> \\[ \\Phi(x) =  \\begin{cases} -\\dfrac{1}{2\\pi} \\ln |x|, \\quad \\text{if } n = 2\\\\ \\dfrac{1}{n(n-2)\\alpha(n)} \\cdot \\dfrac{1}{|x|^{n-2}}, \\quad \\text{if } n \\geq 3 \\end{cases} \\tag{3.3} \\] <p>where \\(\\alpha (n)\\) is the volume of the unit ball in \\(\\mathbb{R}^n\\). We see that \\(\\Phi\\) satisfies Laplace's equation on \\(\\mathbb{R}^n-{0}\\). As we will show in the following claim, \\(\\Phi\\) satisfies \\(-\\Delta_x \\Phi = \\delta_0\\). For this reason, we call \\(\\Phi\\) the fundamental solution of Laplace's equation.</p>"},{"location":"mathematics/poissonEquation/#3-solving-poissons-equation","title":"3. Solving Poisson's Equation","text":"<p>We now return to solving Poisson's Equation.</p> \\[ - \\Delta u = f, \\quad x \\in \\mathbb{R}^n \\] <p>From our discussion before, we expect the function</p> \\[ v(x) \\equiv \\int_{\\mathbb{R}^n} \\Phi(x-y) f(y)\\, dy \\] <p>to give us a solution of Poisson's Equation. We now prove that this is in fact true. First, we make a remark.</p> <p>Remark. If we hope that the function \\(v\\) defined above solves Poisson's equation, we must first verify that integral actually converges. If we assume \\(f\\) has compact support on some bounded set \\(K\\) in \\(\\mathbb{R}^n\\), then we see that</p> \\[ \\int_{\\mathbb{R}^n} \\Phi(x-y)f(y) \\, dy \\leq \\|f\\|_{L^\\infty} \\int_{K} \\big| \\Phi(x-y) \\big| \\, dy \\] <p>If we additionally assume that \\(f\\) is bounded, then \\(\\|f\\|_{L^\\infty} \\leq C\\). It is left as an exercise to verify that</p> \\[ \\int_{K} \\big| \\Phi(x-y) \\big| \\, dy \\le + \\infty \\] <p>on any compact set \\(K\\).</p> <p>Theorem 2.</p> <p>Assume \\(f \\in C^2(\\mathbb{R}^n)\\) and has compact support. Let</p> \\[ u(x) \\equiv \\int_{\\mathbb{R}^n} \\Phi(x-y)f(y) \\, dy \\] <p>where \\(\\Phi\\) is the fundamental solution of Laplace's equation. Then</p> <ol> <li>\\(u \\in C^2(\\mathbb{R}^n)\\)</li> <li>\\(- \\Delta u = f \\quad in \\quad \\mathbb{R}^n\\)</li> </ol> Proof <ol> <li>By a change of variables, we write</li> </ol> \\[ u(x)=\\int_{\\mathbb{R}^n}\\Phi(x-y)f(y)\\,dy = \\int_{\\mathbb{R}^n}\\Phi(y)f(x-y)\\,dy \\] <p>Let \\(e_i=(...,0,1,0,...)\\) be the unit vector in \\(\\mathbb{R}^n\\) with a \\(1\\) in the \\(i^{th}\\) slot. Then</p> \\[ \\frac{u(x+he_i)-u(x)}{h} = \\int_{\\mathbb{R}^n}\\Phi(y)\\left[\\frac{f(x+he_i-y)-f(x-y)}{h}\\right]\\,dy \\] <p>Now \\(f\\in C^2\\) implies</p> \\[ \\frac{f(x+he_i-y)-f(x-y)}{h} \\rightarrow \\frac{\\partial f}{\\partial x_i}(x-y) \\quad \\text{as} \\quad h \\rightarrow 0 \\] <p>uniformly on \\(\\mathbb{R}^n\\). Therefore,</p> \\[ \\frac{\\partial u}{\\partial x_i}(x) = \\int_{\\mathbb{R}^n}\\Phi(y)\\frac{\\partial f}{\\partial x_i}(x-y)\\,dy \\] <p>Similarly,</p> \\[ \\frac{\\partial^2 u}{\\partial x_i\\partial x_j}(x) = \\int_{\\mathbb{R}^n}\\Phi(y)\\frac{\\partial^2 f}{\\partial x_i\\partial x_j}(x-y)\\,dy \\] <p>This function is continuous because the right-hand side is continuous.</p> <ol> <li>By the above calculations and Claim \\(1\\), we see that</li> </ol> \\[ \\begin{aligned} \\Delta_xu(x)&amp;=\\int_{\\mathbb{R}^n}\\Phi(y)\\Delta_xf(x-y)\\,dy \\\\ &amp;=\\int_{\\mathbb{R}^n}\\Phi(y)\\Delta_yf(x-y)\\,dy \\\\ &amp;=-f(x) \\end{aligned} \\]"},{"location":"mathematics/solidAngleEllipse/","title":"Solid Angle of Ellipse","text":"<p>Related Papers</p> <p>Analytical solution for the solid angle subtended at any point by an ellipse via a point source radiation vector potential</p>"},{"location":"mathematics/solidAngleEllipse/#1-problem-description","title":"1. Problem Description","text":"<p>Calculate the (signed) solid angle for an ellipse from the origin \\((0,0,0)\\)</p> <p>In short, complete the function s.t. <pre><code>/**\n * @brief Computes the solid angle for an ellipse from the origin(0,0,0)\n *\n * @param c Center of the ellipse. R^3\n * @param n Normal vector of the ellipse. R^3\n * @param d Direction of the long axis. R^3  ensure that &lt;n, d&gt; = 0\n * @param a Semi-major axis length of the ellipse. R\n * @param b Semi-minor axis length of the ellipse. R ensure that a &gt;= b &gt;= 0\n * @return The solid angle in steradians.\n */\ndouble solid_angle_ellipse(\n    const Eigen::Vector3d &amp;c,\n    const Eigen::Vector3d &amp;n,\n    const Eigen::Vector3d &amp;d,\n    const double a,\n    const double b\n);\n</code></pre></p>"},{"location":"mathematics/solidAngleEllipse/#2-math-expression-needed","title":"2. Math Expression Needed","text":""},{"location":"mathematics/solidAngleEllipse/#21-problem-simplify","title":"2.1 Problem Simplify","text":"<p>We can assume that the Normal vector of the ellipse \\(\\boldsymbol{n}\\) is parallel to the z-axis and the direction of the long axis \\(\\boldsymbol{d}\\) is parallel to the x-axis, because it's easy to apply a coordinate transformation to satisfy this condition.</p> <p>So, in the following expression, we assume that \\(\\boldsymbol{n} = (0,0,1)\\) and \\(\\boldsymbol{d} = (1,0,0)\\)</p> <p>Also, we assume that \\(\\boldsymbol{c} = (p,q,h)\\)</p>"},{"location":"mathematics/solidAngleEllipse/#22-analytical-solution-for-the-coaxial-case","title":"2.2 Analytical solution for the coaxial case","text":"<p>It's easy to calculate the solid angle using the eletric field. As we suppose a point charge is placed at the origin, and the soliet angle problem can be transformed into a calculation of the electric flux.</p> <p>So the answer is</p> \\[ \\Omega(a, b, h) = 2\\pi \\left(1 - \\Lambda_0(\\beta, k)\\right) \\tag{57} \\] <p>where</p> \\[ \\beta = \\arcsin \\sqrt{ \\frac{h^2}{h^2 + b^2} } \\tag{58} \\] \\[ k = \\sqrt{\\frac{a^2-b^2}{h^2+a^2}} \\] <p>and</p> \\[ \\Lambda_0(\\beta,k) = \\frac{2}{\\pi}[\\textbf{E}(k)F(\\beta,k')+\\textbf{K}(k)E(\\beta,k)-\\textbf{K}(k)F(\\beta,k)] \\] <p>where \\(k' \\equiv \\sqrt{1-k^2}\\)</p>"},{"location":"mathematics/solidAngleEllipse/#23-analytical-case-related-to-sections-of-a-right-circular-cone","title":"2.3 Analytical case related to sections of a right circular cone","text":"<p>We can apply a coordiante transformation such that, in the transformerd frame, the \\(z'\\)-axis is aligned with the direction from the ellipse center to the observation point, and the \\(x'\\) axis is aligned with the ellipse's major axis.</p> <p>After this transformation, the resulting elliptical cone can be characterized by its new effective semi-axes, which can then be used to compute the solid angle.</p> <p>And here's the new axis length after transformation.</p> \\[ \\bar{a}^2 = a^2-h^2-p^2 + \\sqrt{(p^2-a^2)^2+2h^2(p^2+a^2)+h^4} \\] \\[ \\bar{b}^2 = 2b^2 \\] \\[ \\bar{h}^2 = h^2+p^2-a^2 + \\sqrt{(p^2-a^2)^2+2h^2(p^2+a^2)+h^4} \\] <p>and then, we can apply the conclusion of Chapter 2.2 to calculate the answer, but here's the point, in this occasion, we're not sure that if \\(a &gt; b\\) or \\(a &lt; b\\), so, we should first swap \\(a\\) and \\(b\\) if \\(a &lt; b\\) before apply the conclusion.</p>"},{"location":"mathematics/solidAngleEllipse/#24-general-solution","title":"2.4 General Solution","text":""},{"location":"mathematics/solidAngleEllipse/#241-rotation-of-the-coordiante-system","title":"2.4.1 Rotation of the coordiante system","text":"<p>In the \\((x,y,z)\\) coordinate system, the sheared core obeys the equation</p> \\[     \\left(\\frac{x}{a} - \\frac{pz}{ah}\\right)^2 +     \\left(\\frac{y}{b} - \\frac{qz}{bh}\\right)^2 -     \\left(\\frac{z}{h}\\right) = 0     \\tag{117} \\] <p>we need to rotate the coordinate system \\((x,y,z)\\) to \\((x',y',z')\\) where the \\(z'\\) axis lies along the center line of the right elliptic cone.</p> <p>This rotation need a right-handed rotation by an angle \\(\\gamma\\) about the \\(y\\)-axis to give a new coordinate system \\((\\hat{x},\\hat{y},\\hat{z})\\) and a additional right-handed rotation by an angle \\(\\psi\\) about the \\(\\hat{x}\\) axis to give the new coordinate system \\((x',y',z')\\)</p> <p>Then, we get</p> \\[ \\begin{bmatrix}   x'\\\\   y'\\\\   z' \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\psi &amp; \\sin\\psi \\\\ 0 &amp; -\\sin\\psi &amp; \\cos\\psi \\end{bmatrix} \\begin{bmatrix} \\cos\\gamma &amp; 0 &amp; -\\sin\\gamma \\\\ 0 &amp; 1 &amp; 0 \\\\ \\sin\\gamma &amp; 0 &amp; \\cos\\gamma \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\tag{119} \\] <p>Substituting Eq.(119) into Eq.(117), expanding and collecting terms gives</p> \\[ A_1x'^2 + B_1x'y' + C_1y'^2+D_1x'+E_1y'=F_1 \\tag{121} \\] <p>where</p> \\[ A_1 = \\frac{1}{a^2} + A \\sin^2\\gamma + B \\sin 2\\gamma \\] \\[ B_1 = 2C \\sin\\gamma \\cos\\psi - 2B \\cos 2\\gamma \\sin\\psi - A \\sin 2\\gamma \\sin\\psi \\] \\[ C_1 = \\frac{\\sin^2 \\psi}{a^2} + \\frac{\\cos^2 \\psi}{b^2} - B \\sin 2\\gamma \\sin^2 \\psi - C \\cos\\gamma \\sin 2\\psi + A \\cos^2\\gamma \\sin^2 \\psi \\] \\[ D_1 = -(A \\sin 2\\gamma \\cos\\psi + 2B \\cos 2\\gamma \\cos\\psi + 2C \\sin\\gamma \\sin\\psi) z' \\] \\[ E_1 = \\left(D \\sin 2\\psi - B \\sin 2\\gamma \\sin 2\\psi + A \\cos^2 \\gamma \\sin 2\\psi - 2C \\cos\\gamma \\cos 2\\psi \\right) z' \\] \\[ F_1 = \\left( B \\sin 2\\gamma \\cos^2 \\psi - \\frac{\\cos^2 \\psi}{a^2} - \\frac{\\sin^2 \\psi}{b^2} - A \\cos^2 \\gamma \\cos^2 \\psi - C \\cos\\gamma \\sin 2\\psi \\right) z'^2 \\] <p>and where</p> \\[ A = \\frac{p^2}{a^2 h^2} + \\frac{q^2}{b^2 h^2} - \\frac{1}{a^2} - \\frac{1}{h^2} \\] \\[ B = \\frac{p h}{a^2 h^2} \\] \\[ C = \\frac{q h}{b^2 h^2} \\] \\[ D = \\frac{1}{a^2} - \\frac{1}{b^2} \\]"},{"location":"mathematics/solidAngleEllipse/#242-solution-for-the-rotation-angle","title":"2.4.2  Solution for the rotation angle","text":"<p>For the \\(z'\\) axis to lie along the axis of the cone, the coefficients \\(D_1\\) and \\(E_1\\) must vanish simultaneously and hence</p> \\[ A \\sin 2\\gamma + 2B \\cos 2\\gamma + 2C \\sin\\gamma \\tan\\psi = 0 \\] \\[ \\left(D - B \\sin 2\\gamma + A \\cos^2 \\gamma \\right) \\tan 2\\psi - 2C \\cos\\gamma = 0 \\] <p>After solving the system of equations, we get</p> \\[ \\gamma_j = \\arctan(w_j) \\] \\[ \\psi_j = -\\arctan\\left( \\frac{A \\sin 2\\gamma_j + 2B \\cos 2\\gamma_j}{2C \\sin \\gamma_j} \\right) \\] <p>where \\(w_j\\), \\(j \\in {1,2,3}\\) is the three roots of the function</p> \\[ w^3 - \\left( \\frac{A}{B} + \\frac{B}{D} + \\frac{C^2}{BD} \\right) w^2 + \\left( \\frac{A}{D} - 1 \\right) w + \\frac{B}{D} = 0 \\] <p>it is proved from the geometry that all three roots are real number.</p> <p>only one root that satisfied \\(B_1^2-4A_1C_1 &lt; 0\\) is the correct root that needed.</p>"},{"location":"mathematics/solidAngleEllipse/#243-equation-of-the-ellipse-in-canonical-form","title":"2.4.3 Equation of the ellipse in canonical form","text":"<p>The value of \\(z'\\) in Eq.(127) for \\(F_1\\) is arbitrary and can be conveniently set to unity.</p> <p>Since \\(E_1\\) and \\(F_1\\) is zero now, we can apply another right-handed rotation by an angle \\(\\lambda\\) about the \\(z'\\) axis to set \\(B_1\\) to zero.</p> <p>We denoting the new rotated coordinates by \\((\\bar{x},\\bar{y},\\bar{z})\\), then do the same steps of the previous part.</p> <p>Then, we can get the final equation of the ellipse in the form</p> \\[ A_2\\bar{x}^2+C_2\\bar{y}^2=F_1 \\] <p>where</p> \\[ A_2 = \\frac{A_1 + C_1 + \\operatorname{sgn}(A_1 - C_1) \\sqrt{B_1^2 + (A_1 - C_1)^2}}{2} \\] <p>and</p> \\[ C_2 = \\frac{A_1 + C_1 - \\operatorname{sgn}(A_1 - C_1) \\sqrt{B_1^2 + (A_1 - C_1)^2}}{2} \\] <p>as the equation of Eq.(57), we can get the final solid angle which is equal to </p> \\[ \\Omega \\left( \\sqrt{\\frac{F_1}{A_2}}, \\sqrt{\\frac{F_1}{C_2}} \\right) \\]"},{"location":"mathematics/solidAngleEllipse/#3-results","title":"3. Results","text":""},{"location":"mathematics/solidAngleEllipse/#31-time-complexity","title":"3.1 time Complexity","text":"<p>The horizontal axis repersents the number of triangles used to divide the ellipse in the numerical intergration, while the vertical axis shows the average time required to compute the solid angle. The orange curve correspoinds to the numerical-integration approach, whereas the red dashed line denotes the analytical formula(\\(1.9123\\mu s\\)).</p>"},{"location":"mathematics/solidAngleEllipse/#32-computational-error","title":"3.2 computational error","text":"<p>The error was determined by comparing our results with those reported in the paper, which are accurate to 20 significant digits The horizontal axis repersents the number of triangles used to divide the ellipse in the numerical intergration, while the vertical axis shows the average error in the calculated solid angle. The orange curve correspoinds to the numerical-integration approach, whereas the red dahed line denotes the analytical formula(\\(9.20974e-15\\)).</p> <p>summery table</p> N Time (\u03bcs) Error 1 0.0798565 8.52715e-01 2 0.101816 6.97061e-01 4 0.144651 4.03314e-01 8 0.247237 9.64311e-02 16 0.426442 2.40481e-02 32 0.814843 3.64688e-03 64 1.57491 7.55273e-05 analytical fomula 1.9123 9.20974e-15 128 3.22987 1.14700e-07 256 6.43509 5.02762e-13 512 13.982 6.69005e-16 1024 24.3732 1.48253e-15 2048 48.5727 8.52766e-16 4096 107.324 2.98612e-15 8192 196.24 3.20553e-15"},{"location":"mathematics/solidAngleEllipse/#33-accelerate","title":"3.3 accelerate","text":"<p>if we disable the elliptic integral promotion to long double, we can get the table following</p> n Time (\u03bcs) Error 1 0.0564791 8.22507e-01 2 0.0774804 1.13996e+00 4 0.126105 3.63865e-01 8 0.223437 1.16768e-01 analytical fomula 0.314463 9.25467e-15 16 0.413136 2.40539e-02 32 0.763126 3.72579e-03 64 1.57758 7.52979e-05 128 3.1183 1.14699e-07 256 6.22602 5.03033e-13 512 13.5295 8.50134e-16 1024 24.4535 1.18320e-15 2048 49.7387 1.77492e-15 4096 127.653 2.76455e-15 8192 209.726 3.45533e-15 <p>if we disable the elliptic integral promotion to double, we can get the table following. But for some extreme samples, analytical fomula error would goto 1e-5, which will maybe not be accpeted.</p> n Time (\u03bcs) Error 1 0.050011 8.22507e-01 2 0.0698501 1.13996e+00 4 0.134739 3.63865e-01 analytical fomula 0.186762 5.19641e-07 8 0.230931 1.16768e-01 16 0.435317 2.40539e-02 32 0.826416 3.72579e-03 64 1.66177 7.52979e-05 128 3.26956 1.14699e-07 256 6.37945 5.03033e-13 512 12.6507 8.50134e-16 1024 24.9748 1.18320e-15 2048 50.179 1.77492e-15 4096 100.874 2.76455e-15 8192 202.717 3.45533e-15"}]}