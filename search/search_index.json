{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#hello-this-is-indexmd","title":"Hello, this is index.md","text":"<p>\\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\), hello</p> <p>\\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\).</p>"},{"location":"AI/TSFM/Kronos/","title":"Kronos: A Foundation Model for the Language of Financial Markets","text":"<p>Reference Paper</p> <p>Kronos: A Foundation Model for the Language of Financial Markets</p> <p>Author: Department of Automation Tsinghua University</p> <p>Date: 2 Aug 2025</p> <p>Abstract</p> <p>The success of large-scale pre-training paradigm, exemplified by Large Language Models (LLMs), has inspired the development of Time Series Foundation Models (TSFMs). However, their application to financial candlestick (K-line) data remains limited, often underperforming non-pre-trained architectures. Moreover, existing TSFMs often overlook crucial downstream tasks such as volatility prediction and synthetic data generation. To address these limitations, we propose Kronos, a unified, scalable pre-training framework tailored to financial K-line modeling. Kronos introduces a specialized tokenizer that discretizes continuous market information into token sequences, preserving both price dynamics and trade activity patterns. We pre-train Kronos using an autoregressive objective on a massive, multi-market corpus of over 12 billion K-line records from 45 global exchanges, enabling it to learn nuanced temporal and cross-asset representations. Kronos excels in a zero-shot setting across a diverse set of financial tasks. On benchmark datasets, Kronos boosts price series forecasting RankIC by 93% over the leading TSFM and 87% over the best non-pre-trained baseline. It also achieves a 9% lower MAE in volatility forecasting and a 22% improvement in generative fidelity for synthetic K-line sequences. These results establish Kronos as a robust, versatile foundation model for end-to-end financial time series analysis. Our pre-trained model is publicly available at https://github.com/shiyu-coder/Kronos.</p>"},{"location":"AI/TSFM/Kronos/#1-introduction","title":"1. Introduction","text":"<p>Financial markets is a critical and challenging application area for TSFMs, given their inherent data richness, high-frequency observations, and complex, non-stationary temporal dynamics. At the core of this domain are K-line sequences, multivariate time series derived from candle-stick charts that record Open, High, Low, and Close prices, along with trading Volume and Amount(Turnover) over fixed intervals(OHLCVA). These sequences constitute a highly compact, information-dense \"language\" through which market participants interpret prices movements, volatility regimes, liquidity shifts, and colletive sentment.</p>"},{"location":"AI/TSFM/Kronos/#2-motivation","title":"2. Motivation","text":"<p>Despite the promise of TSFMs. Old application to finance faces significant hurdles.</p> <ol> <li>Unique statistical properties of K-line data<ul> <li>Low signal-to-noise ratio, strong non-stationarity, and complex interdependencies among OHLCVA features.</li> <li>These properties are poorly aligned with the inductive biases of general-purpose TSFMs.</li> </ul> </li> <li>Underserved financial domain in pre-training<ul> <li>Most TSFMs are trained on broad, multi-domain corpora where financial data accounts for only a tiny fraction.</li> <li>As a result, financial-specific dynamics are overlooked or averaged out.</li> </ul> </li> <li>Task gaps<ul> <li>Critical finance-specific tasks such as volatility prediction, synthetic data generation, and risk management remain largely unaddressed by existing TSFMs.</li> </ul> </li> </ol> <p>Empirical evidence shows that general-purpose TSFMs often underperform even non-pre-trained, specialized models (like iTransformer) on financial tasks. This reveals a gap: the field lacks a foundation model built specifically for financial time series.</p>"},{"location":"AI/TSFM/Kronos/#3-preliminary","title":"3. Preliminary","text":"<p>Let \\(D\\)-dimensional vector \\(x_t \\in \\mathbb{R}^D\\) denote the K-line observation at discrete time \\(t\\), comprising \\(D\\) key financial indicators. In this work, the dimension was fixed \\(D=6\\) to represent OHLCVA attributes. Given a historical sequence \\(x_{1:T}=(x_1,x_2,...,x_T)\\), our objective is to predict the following \\(H\\) observations \\(\\hat{x}_{T+1:T+H}=(\\hat{x}_{T+1}, \\hat{x}_{T+2},...,\\hat{x}_{T+H})\\).</p> <p>Rather than operating on raw continuous inputs, Kronos first quantizes each multivariate observation \\(x_t\\) into a discrete token \\(b_t\\) via a learnable codebook \\(\\mathcal{C}\\). Consequently, the original sequence \\(x_{1:T}=(x_1,x_2,...,x_T)\\) is mapped to \\(b_{1:T}=(b_1,...,b_T)\\). The forecasting task then reduces to an autoregressive token-sequence modeling problem:</p> \\[ p(\\mathbf{b}_{T+1:T+H}|\\mathbf{b}_{1:T}) = \\sum_{h=1}^{H} p(b_{T+h}|\\mathbf{b}_{1:T+h-1}) \\]"},{"location":"AI/TSFM/Kronos/#4-methodology","title":"4. Methodology","text":"<p>Kronos implements this via a two-phase framework: (1) K-line Tokenization and (2) Autoregressive Pre-training</p>"},{"location":"AI/TSFM/Kronos/#41-k-line-tokenization","title":"4.1 K-line Tokenization","text":"<p>The tokenization is achieved using a Transformer-based autoencoder composed of an encoder \\(E_{\\text{enc}}\\), a quantizer \\(Q\\), and a decoder \\(E_{\\text{dec}}\\).</p> <p>Kronos adapt Binary Spherical Quantization(BSQ) for this task. BSQ quantizes a continuous latent vector \\(\\xi_t\\) into a \\(k\\)-bit binary code \\(b_t \\in \\{-1,1\\}^k\\) by projecting it onto a set of learnable hyperplanes.</p>"},{"location":"AI/TSFM/Kronos/#411-introduction-to-bsq","title":"4.1.1 Introduction to BSQ","text":"<p>Related Paper</p> <p>Image and Video Tokenization with Binary Spherical Quantization</p> <p>Binary Spherical Quantization (BSQ) optimizes over an implicit codebook \\(\\textbf{C}_{BSQ}=\\{-\\frac{1}{\\sqrt{L}}, \\frac{1}{\\sqrt{L}}\\}^L\\), a hypercube projected onto a unit sphere. Each corner \\(\\textbf{c}_k \\in \\textbf{C}_{BSQ}\\) of a hypercube corresponds to a unique token \\(k\\). The quantizer works as follows: it projects some high-dimensional latent embedding \\(\\textbf{z}\\) to a lower-dimensional unit hypersphere \\(\\textbf{u}\\), applies binary quantization per axis \\(\\hat{\\textbf{u}}=sign(\\textbf{u})\\) and back-projects to the quantized vector in the original latent space \\(\\hat{\\textbf{x}}\\). Specifically, we start with an encoded visual input \\(\\textbf{z}=\\mathcal{E}(\\textbf{x}) \\in \\mathbb{R}^d\\). We first linearly project the latent embedding to \\(L\\) dimensions \\(\\textbf{v} = Linear(\\textbf{z}) \\in \\mathbb{R}^L\\), where \\(L \\ll d\\). Next, we obtain project \\(\\textbf{v}\\) onto the unit sphere \\(\\textbf{u}=\\frac{\\textbf{v}}{|\\textbf{v}|}\\) and perform binary quantization to each dimension of \\(u\\) independently \\(\\hat{\\textbf{u}}=\\frac{1}{\\sqrt{L}} sign(\\textbf{\\textbf{u}})\\), where \\(sign(x)\\) is the sign function. To keep outputs on the unit sphere, we map \\(sign(0) \\rightarrow 1\\). We use a Straight-Through Estimator (STE) make the operator differentiable, \\(sign_{STE}(x) = sg(sign(x)\u2212x) + x\\), where \\(sg(\u00b7)\\) denotes the stop-gradient operation. Finally, we back-project the quantized \\(\\hat{\\textbf{u}}\\) to the \\(d\\)-dimensional space \\(\\hat{\\textbf{z}} = Linear(\\hat{\\textbf{u}}) \\in \\mathbb{R}^d\\).</p>"},{"location":"AI/TSFM/Kronos/#412-effectiveness-of-bsq","title":"4.1.2 Effectiveness of BSQ","text":"<p>The effectiveness of BSQ tokenizer can be analyzed from two key perspectivs: its inherent noise suppression and its ability to create a structured, discrete state space suitable for sequence modeling.</p>"},{"location":"AI/TSFM/Kronos/#noise-suppression-and-stability","title":"Noise Suppression And Stability","text":"<p>BSQ's projection of embeddings onto a unit sphere prior to binarization guarantees that the expected distortion is strictly upper-bounded:</p> \\[ \\mathbb{E}_u\\|u-\\hat{u}\\| &lt; \\sqrt{2-\\frac{2}{\\sqrt{L}}} &lt; \\sqrt{2} \\] <p>This bound tightens as the codebook dimension \\(L\\) increases. In contrast, simple methods like sign-based quantization without normalization(e.g., LFQ) lack such a guarantee, leaving them vulnerable to arbitarily large errors from outlier inputs.</p>"},{"location":"AI/TSFM/Kronos/#learning-in-a-compact-and-discrete-state-space","title":"Learning in a Compact and Discrete State Space","text":"<p>High-frequency financial data exists in a high-dimensional, continuous state space. This tokenizer maps these in a finite, discrete vocabulary of tokens. This discretization serves as a powerful form of regularization with two main benefits.</p> <p>Inproved Sample Efficiency and Generalization: Instead of learning a complex function over a continuous space, a downstream model like a Transformer learns to predict transitions and patterns among a finite set of abstract states(tokens). This simplifies the learning tasks and also allows the model to learn robust patterns from fewer examples, which is particularly critical for modeling rare market phenomena.</p> <p>Reduced Overfitting: The quantization process inherently discards fine-grained, potentially noisy variations within each quantization cells. This prevents the model from fitting to spurious artifacts in the tarining data.</p>"},{"location":"AI/TSFM/Kronos/#hyperspherical-geometry-for-tail-sensitivity","title":"Hyperspherical geometry for tail sensitivity","text":"<p>In financial contexts, market returns and price changes often exhibit heavy tails(or fat tails). The heavy-tail distribution of price changes is one of the key sources of trading profits in quantitative investment and cannot be ignored.</p> <p>Unlike standard vector-quantization on the Euclidean sphere, BSQ's binary encoding preserves angular information very efficiently, making it more sensitive to fat-tail data that manifest as sharp directional changes in feature space. This aligns well how microstructure events often appear as abrupt shifts in the \"direction\" of the joint price-volume vector.</p>"},{"location":"AI/TSFM/Kronos/#413-subtoken-factorization","title":"4.1.3 Subtoken Factorization","text":"<p>While a large number of bits \\(k\\) (e.g., \\(k=20\\)) is desirable for capturing rich financial patterns, it results in an exponentially large vocabulary of size \\(2^k\\). To mitigate this, we use the subtoken factorization and factorize the \\(k\\)-bit code into \\(n\\) subspaces. Motivated by the trade-off between parameter savings and latency costs, we set \\(n=2\\). We partition the code into a coarse subtoken \\(b_t^c\\) and a fine subtoken \\(b_t^f\\) of equal bit length. The resulting code \\(b_t\\) is a concatenation of these two subtokens: \\(b_t=[b_t^c,b_t^f]\\).</p> <p>We train the tokenizer with a composite objective that combines a hierachical reconstruction loss and a commitment loss for BSQ:</p> \\[ \\mathcal{L}_{\\text{tokenizer}} = \\mathcal{L}_{\\text{coarse}} + \\mathcal{L}_{\\text{fine}} + \\lambda \\mathcal{L}_{\\text{quant}} \\] <p>where \\(\\lambda\\) is a balancing hyperparameter. The components are defined as:</p> <ul> <li>\\(\\mathcal{L}_{\\text{coarse}}=\\mathbb{E}[\\|\\textbf{x}-E_{\\text{dec}}(\\textbf{b}^c)\\|^2]\\), which trains the coarse subtoken \\(\\textbf{b}^c\\) to form a low-fidelity reconstruction.</li> <li>\\(\\mathcal{L}_{\\text{fine}}=\\mathbb{E}[\\|\\textbf{x}-E_{\\text{dec}}(\\textbf{b})\\|^2]\\), which evaluates the high-fidelity reconstruction using the complete token \\(\\textbf{b}\\).</li> <li>\\(\\mathcal{L}_{\\text{quant}}\\) is the quantization loss from BSQ that regularizes the learning process. It penalizes the L2 distance between continuous latent vector \\(\\xi\\) and their binary codes \\(\\textbf{b}\\), aligning the encoder's outputs with the learned codebook to ensure stable training.</li> </ul>"},{"location":"AI/TSFM/Kronos/#42-hierarchical-autoregressive-modeling","title":"4.2 Hierarchical Autoregressive Modeling","text":"<p>The primary objective is to estimate the joint distribution over the token sequence \\(\\textbf{b}=\\{b_1,...,b_r\\}\\). A simplified form of Equation 1 can be derived as:</p> \\[ p(\\textbf{b}) = \\sum_{t=1}^Tp(b_t|\\textbf{b}_{&lt;t}) \\] <p>where \\(\\textbf{b}_{&lt;t}\\) denotes all preceding tokens up to time \\(t-1\\).</p> <p>Given the hierarchical token design, in which each token is structured as \\(b_t=[b_t^c,b_t^f]\\), we further decompose the conditional probability using the chain rule to explicitly capture the inherent coarse-to-fine dependency:</p> \\[ p(b_t|\\textbf{b}_{&lt;t})=p(b_t^c|\\textbf{b}_{&lt;t}) \\cdot p(b_t^f|\\textbf{b}_{&lt;t}, b_t^c) \\] <p>At time \\(i\\), the subtokens \\(b_i^c\\) and \\(b_i^f\\) are independently projected into vector representations using two distinct embedding layers, resulting in representations \\(e_c(b_i^c)\\) and \\(e_f(b_i^f)\\), respectively. These embeddings are then concatenated and linearly projected to produce a fused input vector:</p> \\[ \\text{v}_i = W_{fuse}([e_c(b_i^c); e_f(b_i^f)]) \\] <p>where \\([\\cdot ; \\cdot]\\) denotes concatenation, and \\(W_{fuse}\\) is a learnable weight matrix responsible for projecting the combined representaion into the model's latent space.</p>"},{"location":"AI/TSFM/Kronos/#coarse-subtoken-prediction","title":"Coarse Subtoken Prediction","text":"<p>The history vector \\(\\textbf{h}_t\\) is projected by a linear head \\(W_c\\) to produce logits for the first subtoken's distribution:</p> \\[ p(b_t^c|\\textbf{b}_{&lt;c})=\\text{softmax}(W_c\\textbf{h}_t) \\]"},{"location":"AI/TSFM/Kronos/#fine-subtoken-prediction","title":"Fine Subtoken Prediction","text":"<p>During the trainiing, we use the model's own prediction from the previous step, \\(\\hat{b}_t^c\\), which is sampled from the predicted distribution \\(p(b_t^c|\\textbf{b}_{&lt;t})\\), rather than using the ground-truth subtoken(i.e., teacher-forcing). We use a cross-attention mechanism where the embedding of \\(\\hat{b}_t^c\\) acts as the query, and the history \\(\\textbf{h}_t\\) provides the key and value. The result is projected by the second head \\(W_f\\):</p> \\[ \\textbf{h}_t^{\\text{update}} = \\text{CrossAttn}(q=e_c(\\hat{b}_t^c), k=v=\\textbf{h}_t) \\] \\[ p(b_t^f|\\textbf{b}_{&lt;t},b_t^c)=\\text{softmax}(W_f\\textbf{h}_t^{\\text{update}}) \\] <p>The overall training objective \\(\\mathcal{L}_{ar}\\) is the negative log-likelihood of the data, summed over both prediction steps:</p> \\[ \\mathcal{L}_{\\text{ar}}=-\\mathbb{E}_{\\textbf{b}\\sim\\mathcal{D}} \\sum_{t=1}^T\\left[\\log p(b_t^c|\\textbf{b}_{&lt;t})+\\log p(b_t^f|\\textbf{b}_{&lt;t}, b_t^c)\\right] \\] <p>where \\(\\mathcal{D}\\) represents the data distribution.</p>"},{"location":"AI/TSFM/Kronos/#5-results-and-conclusion","title":"5. Results and Conclusion","text":""},{"location":"mathematics/","title":"Mathematics","text":"<p>Here's the section about math, where all the math knowledge needed located.</p>"},{"location":"mathematics/poissonEquation/","title":"Poisson's Equation","text":"<p>Related Works</p> <p>Stanford course Math 220B</p>"},{"location":"mathematics/poissonEquation/#1-problem-description","title":"1. Problem Description","text":"<p>We want to solve the Laplace's equation</p> \\[ \\Delta u = 0 \\] <p>and it's inhomogeneous versions, Poisson's equation</p> \\[ - \\Delta u = f \\] <p>We say a function u satisfying Laplace's equation is a harmonic function.</p>"},{"location":"mathematics/poissonEquation/#2-the-fundamental-solution","title":"2. The Fundamental Solution","text":"<p>Consider Laplace's equation in \\(\\mathbb{R}^n\\),</p> \\[ \\Delta u = 0 \\quad x \\in \\mathbb{R}^n \\] <p>Clearly, there are a lot of functions u which satisfy this equation. In particular, any constant function is harmonic. In addition, any function of the form \\(u(x)=a_1x_1 + ... + a_nx_n\\) for constants \\(a_i\\) is also a solution. Of course, we can list a number of others. Here, however, we are intrested in finding a particular solution of Laplace's equation which will allow us to solve Poisson's equation.</p> <p>Give the symmertic nature of Laplace's equation, we look for a radial solution. That is, we look for a harmonic funtion \\(u\\) on \\(\\mathbb{R}^n\\) such that \\(u(x)=v(|x|)\\). In addition, to being a natural choice due to the symmetry of Laplace's equation, radial solutions are natural to look for because they reduce a PDE to an ODE, which is generally easier to solve. Therefore, we look for a radial solution.</p> <p>If \\(u(x)=v(|x|)\\), then</p> \\[ u_{x_i} = \\frac{x_i}{|x|}v'(|x|) \\quad |x| \\neq 0 \\] <p>which implies</p> \\[ u_{x_ix_i} = \\frac{1}{|x|}v'(|x|) - \\frac{x_i^2}{|x|^3}v'(|x|) + \\frac{x_i^2}{|x|^2}v''(|x|) \\quad |x| \\neq 0 \\] <p>Therefore,</p> \\[ \\Delta u = \\frac{n-1}{|x|}v'(|x|) + v''(|x|) \\] <p>Letting \\(r = |x|\\), we see that \\(u(x)=v(|x|)\\) is a radial solution of Laplace's equation implies \\(v\\) satisfies</p> \\[ \\frac{n-1}{r}v'(r)+v''(r) = 0 \\] <p>And then, from some calculations, we see that for any constants \\(c1\\), \\(c2\\), the function</p> \\[ u(x) \\equiv \\begin{cases} c_1 \\ln |x| + c_2, &amp; \\text{if } n = 2 \\\\ \\frac{c_1}{(2 - n)|x|^{n - 2}} + c_2, &amp; \\text{if } n \\geq 3 \\end{cases} \\tag{3.1} \\] <p>for \\(x \\in \\mathbb{R}^n\\), \\(|x| \\neq 0\\) is a solution of Laplace's equation in \\(\\mathbb{R}^n - {0}\\). We notice that the function \\(u\\) defined in (3.1) satisfies \\(\\Delta u(x)=0\\) for \\(x \\neq 0\\), but at \\(x=0\\), \\(\\Delta u(0)\\) is undefined. We claim that we can choose constants \\(c_1\\) and \\(c_2\\) appropriately so that</p> \\[ - \\Delta_x u = \\delta_0 \\] <p>in the sense of distributions. Recall that \\(\\delta_0\\) is the distribution which is defined as follows. For all \\(\\phi \\in \\mathbb{D}\\),</p> \\[ (\\delta_0, \\phi) = \\phi(0) \\] <p>Claim 1</p> <p>For \\(\\Phi\\) defined in (3.3), \\(\\Phi\\) satisfies</p> \\[ -\\Delta_x\\Phi = \\delta_0 \\] <p>in the sense of distributions. That is, for all \\(g\\in \\mathcal{D}\\),</p> \\[ -\\int_{\\mathbb{R}^n}\\Phi(x)\\Delta_xg(x)\\,dx=g(0) \\] Proof <p>The proof of this claim is a little complex. You can reference to this handouts: Stanford course Math 220B laplace</p> <p>For now, though, assume we can prove this. That is, assume we can find constants \\(c_1\\),\\(c_2\\) such that \\(u\\) defined in (3.1) satisfied</p> \\[ - \\Delta_x u = \\delta_0 \\tag{3.2} \\] <p>Let \\(\\Phi\\) denote the solution of (3.2). Then, define</p> \\[ v(x) = \\int_{\\mathbb{R}^n} \\Phi(x - y) f(y) \\, dy \\] <p>\\(Formally\\), we compute the Laplacian of \\(v\\) as follows,</p> \\[ \\begin{aligned} - \\Delta_x v &amp;= - \\int_{\\mathbb{R}^n} \\Delta_x \\Phi(x - y) f(y) \\, dy \\\\              &amp;= - \\int_{\\mathbb{R}^n} \\Delta_y \\Phi(x - y) f(y) \\, dy \\\\              &amp;= \\int_{\\mathbb{R}^n} \\delta_x f(y) \\, dy = f(x) \\end{aligned} \\] <p>That is, \\(v\\) is a solution of Poisson's equation! Of course, this set of equalities above is entirely formal. We have not prove anything yet. However, we have motivated a solution formula for Poisson's equation from a solution to (3.2). We now return to using the radial solution (3.1) to find a solution of (3.2).</p> <p>Define the function \\(\\Phi\\) as follows. For \\(|x| \\neq 0\\), let</p> \\[ \\Phi(x) =  \\begin{cases} -\\dfrac{1}{2\\pi} \\ln |x|, \\quad \\text{if } n = 2\\\\ \\dfrac{1}{n(n-2)\\alpha(n)} \\cdot \\dfrac{1}{|x|^{n-2}}, \\quad \\text{if } n \\geq 3 \\end{cases} \\tag{3.3} \\] <p>where \\(\\alpha (n)\\) is the volume of the unit ball in \\(\\mathbb{R}^n\\). We see that \\(\\Phi\\) satisfies Laplace's equation on \\(\\mathbb{R}^n-{0}\\). As we will show in the following claim, \\(\\Phi\\) satisfies \\(-\\Delta_x \\Phi = \\delta_0\\). For this reason, we call \\(\\Phi\\) the fundamental solution of Laplace's equation.</p>"},{"location":"mathematics/poissonEquation/#3-solving-poissons-equation","title":"3. Solving Poisson's Equation","text":"<p>We now return to solving Poisson's Equation.</p> \\[ - \\Delta u = f, \\quad x \\in \\mathbb{R}^n \\] <p>From our discussion before, we expect the function</p> \\[ v(x) \\equiv \\int_{\\mathbb{R}^n} \\Phi(x-y) f(y)\\, dy \\] <p>to give us a solution of Poisson's Equation. We now prove that this is in fact true. First, we make a remark.</p> <p>Remark. If we hope that the function \\(v\\) defined above solves Poisson's equation, we must first verify that integral actually converges. If we assume \\(f\\) has compact support on some bounded set \\(K\\) in \\(\\mathbb{R}^n\\), then we see that</p> \\[ \\int_{\\mathbb{R}^n} \\Phi(x-y)f(y) \\, dy \\leq \\|f\\|_{L^\\infty} \\int_{K} \\big| \\Phi(x-y) \\big| \\, dy \\] <p>If we additionally assume that \\(f\\) is bounded, then \\(\\|f\\|_{L^\\infty} \\leq C\\). It is left as an exercise to verify that</p> \\[ \\int_{K} \\big| \\Phi(x-y) \\big| \\, dy \\le + \\infty \\] <p>on any compact set \\(K\\).</p> <p>Theorem 2.</p> <p>Assume \\(f \\in C^2(\\mathbb{R}^n)\\) and has compact support. Let</p> \\[ u(x) \\equiv \\int_{\\mathbb{R}^n} \\Phi(x-y)f(y) \\, dy \\] <p>where \\(\\Phi\\) is the fundamental solution of Laplace's equation. Then</p> <ol> <li>\\(u \\in C^2(\\mathbb{R}^n)\\)</li> <li>\\(- \\Delta u = f \\quad in \\quad \\mathbb{R}^n\\)</li> </ol> Proof <ol> <li>By a change of variables, we write</li> </ol> \\[ u(x)=\\int_{\\mathbb{R}^n}\\Phi(x-y)f(y)\\,dy = \\int_{\\mathbb{R}^n}\\Phi(y)f(x-y)\\,dy \\] <p>Let \\(e_i=(...,0,1,0,...)\\) be the unit vector in \\(\\mathbb{R}^n\\) with a \\(1\\) in the \\(i^{th}\\) slot. Then</p> \\[ \\frac{u(x+he_i)-u(x)}{h} = \\int_{\\mathbb{R}^n}\\Phi(y)\\left[\\frac{f(x+he_i-y)-f(x-y)}{h}\\right]\\,dy \\] <p>Now \\(f\\inC^2\\) implies</p> \\[ \\frac{f(x+he_i-y)-f(x-y)}{h} \\rightarrow \\frac{\\partial f}{\\partial x_i}(x-y) as h \\rightarrow 0 \\] <p>uniformly on \\(\\mathbb{R}^n\\). Therefore,</p> \\[ \\frac{\\partial u}{\\partial x_i}(x) = \\int_{\\mathbb{R}^n}\\Phi(y)\\frac{\\partial f}{\\partial x_i}(x-y)\\,dy \\] <p>Similarly,</p> \\[ \\frac{\\partial^2 u}{\\partial x_i\\partial x_j}(x) = \\int_{\\mathbb{R}^n}\\Phi(y)\\frac{\\partial^2 f}{\\partial x_i\\partial x_j}(x-y)\\,dy \\] <p>This function is continuous because the right-hand side is continuous.</p> <ol> <li>By the above calculations and Claim \\(1\\), we see that</li> </ol> \\[ \\begin{aligned} \\Delta_xu(x)&amp;=\\int_{\\mathbb{R}^n}\\Phi(y)\\Delta_xf(x-y)\\,dy \\\\ &amp;=\\int_{\\mathbb{R}^n}\\Phi(y)\\Delta_yf(x-y)\\,dy \\\\ &amp;=-f(x) \\end{aligned} \\]"},{"location":"mathematics/solidAngleEllipse/","title":"Solid Angle of Ellipse","text":"<p>Related Papers</p> <p>Analytical solution for the solid angle subtended at any point by an ellipse via a point source radiation vector potential</p>"},{"location":"mathematics/solidAngleEllipse/#1-problem-description","title":"1. Problem Description","text":"<p>Calculate the (signed) solid angle for an ellipse from the origin \\((0,0,0)\\)</p> <p>In short, complete the function s.t. <pre><code>/**\n * @brief Computes the solid angle for an ellipse from the origin(0,0,0)\n *\n * @param c Center of the ellipse. R^3\n * @param n Normal vector of the ellipse. R^3\n * @param d Direction of the long axis. R^3  ensure that &lt;n, d&gt; = 0\n * @param a Semi-major axis length of the ellipse. R\n * @param b Semi-minor axis length of the ellipse. R ensure that a &gt;= b &gt;= 0\n * @return The solid angle in steradians.\n */\ndouble solid_angle_ellipse(\n    const Eigen::Vector3d &amp;c,\n    const Eigen::Vector3d &amp;n,\n    const Eigen::Vector3d &amp;d,\n    const double a,\n    const double b\n);\n</code></pre></p>"},{"location":"mathematics/solidAngleEllipse/#2-math-expression-needed","title":"2. Math Expression Needed","text":""},{"location":"mathematics/solidAngleEllipse/#21-problem-simplify","title":"2.1 Problem Simplify","text":"<p>We can assume that the Normal vector of the ellipse \\(\\boldsymbol{n}\\) is parallel to the z-axis and the direction of the long axis \\(\\boldsymbol{d}\\) is parallel to the x-axis, because it's easy to apply a coordinate transformation to satisfy this condition.</p> <p>So, in the following expression, we assume that \\(\\boldsymbol{n} = (0,0,1)\\) and \\(\\boldsymbol{d} = (1,0,0)\\)</p> <p>Also, we assume that \\(\\boldsymbol{c} = (p,q,h)\\)</p>"},{"location":"mathematics/solidAngleEllipse/#22-analytical-solution-for-the-coaxial-case","title":"2.2 Analytical solution for the coaxial case","text":"<p>It's easy to calculate the solid angle using the eletric field. As we suppose a point charge is placed at the origin, and the soliet angle problem can be transformed into a calculation of the electric flux.</p> <p>So the answer is</p> \\[ \\Omega(a, b, h) = 2\\pi \\left(1 - \\Lambda_0(\\beta, k)\\right) \\tag{57} \\] <p>where</p> \\[ \\beta = \\arcsin \\sqrt{ \\frac{h^2}{h^2 + b^2} } \\tag{58} \\] \\[ k = \\sqrt{\\frac{a^2-b^2}{h^2+a^2}} \\] <p>and</p> \\[ \\Lambda_0(\\beta,k) = \\frac{2}{\\pi}[\\textbf{E}(k)F(\\beta,k')+\\textbf{K}(k)E(\\beta,k)-\\textbf{K}(k)F(\\beta,k)] \\] <p>where \\(k' \\equiv \\sqrt{1-k^2}\\)</p>"},{"location":"mathematics/solidAngleEllipse/#23-analytical-case-related-to-sections-of-a-right-circular-cone","title":"2.3 Analytical case related to sections of a right circular cone","text":"<p>We can apply a coordiante transformation such that, in the transformerd frame, the \\(z'\\)-axis is aligned with the direction from the ellipse center to the observation point, and the \\(x'\\) axis is aligned with the ellipse's major axis.</p> <p>After this transformation, the resulting elliptical cone can be characterized by its new effective semi-axes, which can then be used to compute the solid angle.</p> <p>And here's the new axis length after transformation.</p> \\[ \\bar{a}^2 = a^2-h^2-p^2 + \\sqrt{(p^2-a^2)^2+2h^2(p^2+a^2)+h^4} \\] \\[ \\bar{b}^2 = 2b^2 \\] \\[ \\bar{h}^2 = h^2+p^2-a^2 + \\sqrt{(p^2-a^2)^2+2h^2(p^2+a^2)+h^4} \\] <p>and then, we can apply the conclusion of Chapter 2.2 to calculate the answer, but here's the point, in this occasion, we're not sure that if \\(a &gt; b\\) or \\(a &lt; b\\), so, we should first swap \\(a\\) and \\(b\\) if \\(a &lt; b\\) before apply the conclusion.</p>"},{"location":"mathematics/solidAngleEllipse/#24-general-solution","title":"2.4 General Solution","text":""},{"location":"mathematics/solidAngleEllipse/#241-rotation-of-the-coordiante-system","title":"2.4.1 Rotation of the coordiante system","text":"<p>In the \\((x,y,z)\\) coordinate system, the sheared core obeys the equation</p> \\[     \\left(\\frac{x}{a} - \\frac{pz}{ah}\\right)^2 +     \\left(\\frac{y}{b} - \\frac{qz}{bh}\\right)^2 -     \\left(\\frac{z}{h}\\right) = 0     \\tag{117} \\] <p>we need to rotate the coordinate system \\((x,y,z)\\) to \\((x',y',z')\\) where the \\(z'\\) axis lies along the center line of the right elliptic cone.</p> <p>This rotation need a right-handed rotation by an angle \\(\\gamma\\) about the \\(y\\)-axis to give a new coordinate system \\((\\hat{x},\\hat{y},\\hat{z})\\) and a additional right-handed rotation by an angle \\(\\psi\\) about the \\(\\hat{x}\\) axis to give the new coordinate system \\((x',y',z')\\)</p> <p>Then, we get</p> \\[ \\begin{bmatrix}   x'\\\\   y'\\\\   z' \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\psi &amp; \\sin\\psi \\\\ 0 &amp; -\\sin\\psi &amp; \\cos\\psi \\end{bmatrix} \\begin{bmatrix} \\cos\\gamma &amp; 0 &amp; -\\sin\\gamma \\\\ 0 &amp; 1 &amp; 0 \\\\ \\sin\\gamma &amp; 0 &amp; \\cos\\gamma \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\tag{119} \\] <p>Substituting Eq.(119) into Eq.(117), expanding and collecting terms gives</p> \\[ A_1x'^2 + B_1x'y' + C_1y'^2+D_1x'+E_1y'=F_1 \\tag{121} \\] <p>where</p> \\[ A_1 = \\frac{1}{a^2} + A \\sin^2\\gamma + B \\sin 2\\gamma \\] \\[ B_1 = 2C \\sin\\gamma \\cos\\psi - 2B \\cos 2\\gamma \\sin\\psi - A \\sin 2\\gamma \\sin\\psi \\] \\[ C_1 = \\frac{\\sin^2 \\psi}{a^2} + \\frac{\\cos^2 \\psi}{b^2} - B \\sin 2\\gamma \\sin^2 \\psi - C \\cos\\gamma \\sin 2\\psi + A \\cos^2\\gamma \\sin^2 \\psi \\] \\[ D_1 = -(A \\sin 2\\gamma \\cos\\psi + 2B \\cos 2\\gamma \\cos\\psi + 2C \\sin\\gamma \\sin\\psi) z' \\] \\[ E_1 = \\left(D \\sin 2\\psi - B \\sin 2\\gamma \\sin 2\\psi + A \\cos^2 \\gamma \\sin 2\\psi - 2C \\cos\\gamma \\cos 2\\psi \\right) z' \\] \\[ F_1 = \\left( B \\sin 2\\gamma \\cos^2 \\psi - \\frac{\\cos^2 \\psi}{a^2} - \\frac{\\sin^2 \\psi}{b^2} - A \\cos^2 \\gamma \\cos^2 \\psi - C \\cos\\gamma \\sin 2\\psi \\right) z'^2 \\] <p>and where</p> \\[ A = \\frac{p^2}{a^2 h^2} + \\frac{q^2}{b^2 h^2} - \\frac{1}{a^2} - \\frac{1}{h^2} \\] \\[ B = \\frac{p h}{a^2 h^2} \\] \\[ C = \\frac{q h}{b^2 h^2} \\] \\[ D = \\frac{1}{a^2} - \\frac{1}{b^2} \\]"},{"location":"mathematics/solidAngleEllipse/#242-solution-for-the-rotation-angle","title":"2.4.2  Solution for the rotation angle","text":"<p>For the \\(z'\\) axis to lie along the axis of the cone, the coefficients \\(D_1\\) and \\(E_1\\) must vanish simultaneously and hence</p> \\[ A \\sin 2\\gamma + 2B \\cos 2\\gamma + 2C \\sin\\gamma \\tan\\psi = 0 \\] \\[ \\left(D - B \\sin 2\\gamma + A \\cos^2 \\gamma \\right) \\tan 2\\psi - 2C \\cos\\gamma = 0 \\] <p>After solving the system of equations, we get</p> \\[ \\gamma_j = \\arctan(w_j) \\] \\[ \\psi_j = -\\arctan\\left( \\frac{A \\sin 2\\gamma_j + 2B \\cos 2\\gamma_j}{2C \\sin \\gamma_j} \\right) \\] <p>where \\(w_j\\), \\(j \\in {1,2,3}\\) is the three roots of the function</p> \\[ w^3 - \\left( \\frac{A}{B} + \\frac{B}{D} + \\frac{C^2}{BD} \\right) w^2 + \\left( \\frac{A}{D} - 1 \\right) w + \\frac{B}{D} = 0 \\] <p>it is proved from the geometry that all three roots are real number.</p> <p>only one root that satisfied \\(B_1^2-4A_1C_1 &lt; 0\\) is the correct root that needed.</p>"},{"location":"mathematics/solidAngleEllipse/#243-equation-of-the-ellipse-in-canonical-form","title":"2.4.3 Equation of the ellipse in canonical form","text":"<p>The value of \\(z'\\) in Eq.(127) for \\(F_1\\) is arbitrary and can be conveniently set to unity.</p> <p>Since \\(E_1\\) and \\(F_1\\) is zero now, we can apply another right-handed rotation by an angle \\(\\lambda\\) about the \\(z'\\) axis to set \\(B_1\\) to zero.</p> <p>We denoting the new rotated coordinates by \\((\\bar{x},\\bar{y},\\bar{z})\\), then do the same steps of the previous part.</p> <p>Then, we can get the final equation of the ellipse in the form</p> \\[ A_2\\bar{x}^2+C_2\\bar{y}^2=F_1 \\] <p>where</p> \\[ A_2 = \\frac{A_1 + C_1 + \\operatorname{sgn}(A_1 - C_1) \\sqrt{B_1^2 + (A_1 - C_1)^2}}{2} \\] <p>and</p> \\[ C_2 = \\frac{A_1 + C_1 - \\operatorname{sgn}(A_1 - C_1) \\sqrt{B_1^2 + (A_1 - C_1)^2}}{2} \\] <p>as the equation of Eq.(57), we can get the final solid angle which is equal to </p> \\[ \\Omega \\left( \\sqrt{\\frac{F_1}{A_2}}, \\sqrt{\\frac{F_1}{C_2}} \\right) \\]"},{"location":"mathematics/solidAngleEllipse/#3-results","title":"3. Results","text":""},{"location":"mathematics/solidAngleEllipse/#31-time-complexity","title":"3.1 time Complexity","text":"<p>The horizontal axis repersents the number of triangles used to divide the ellipse in the numerical intergration, while the vertical axis shows the average time required to compute the solid angle. The orange curve correspoinds to the numerical-integration approach, whereas the red dashed line denotes the analytical formula(\\(1.9123\\mu s\\)).</p>"},{"location":"mathematics/solidAngleEllipse/#32-computational-error","title":"3.2 computational error","text":"<p>The error was determined by comparing our results with those reported in the paper, which are accurate to 20 significant digits The horizontal axis repersents the number of triangles used to divide the ellipse in the numerical intergration, while the vertical axis shows the average error in the calculated solid angle. The orange curve correspoinds to the numerical-integration approach, whereas the red dahed line denotes the analytical formula(\\(9.20974e-15\\)).</p> <p>summery table</p> N Time (\u03bcs) Error 1 0.0798565 8.52715e-01 2 0.101816 6.97061e-01 4 0.144651 4.03314e-01 8 0.247237 9.64311e-02 16 0.426442 2.40481e-02 32 0.814843 3.64688e-03 64 1.57491 7.55273e-05 analytical fomula 1.9123 9.20974e-15 128 3.22987 1.14700e-07 256 6.43509 5.02762e-13 512 13.982 6.69005e-16 1024 24.3732 1.48253e-15 2048 48.5727 8.52766e-16 4096 107.324 2.98612e-15 8192 196.24 3.20553e-15"},{"location":"mathematics/solidAngleEllipse/#33-accelerate","title":"3.3 accelerate","text":"<p>if we disable the elliptic integral promotion to long double, we can get the table following</p> n Time (\u03bcs) Error 1 0.0564791 8.22507e-01 2 0.0774804 1.13996e+00 4 0.126105 3.63865e-01 8 0.223437 1.16768e-01 analytical fomula 0.314463 9.25467e-15 16 0.413136 2.40539e-02 32 0.763126 3.72579e-03 64 1.57758 7.52979e-05 128 3.1183 1.14699e-07 256 6.22602 5.03033e-13 512 13.5295 8.50134e-16 1024 24.4535 1.18320e-15 2048 49.7387 1.77492e-15 4096 127.653 2.76455e-15 8192 209.726 3.45533e-15 <p>if we disable the elliptic integral promotion to double, we can get the table following. But for some extreme samples, analytical fomula error would goto 1e-5, which will maybe not be accpeted.</p> n Time (\u03bcs) Error 1 0.050011 8.22507e-01 2 0.0698501 1.13996e+00 4 0.134739 3.63865e-01 analytical fomula 0.186762 5.19641e-07 8 0.230931 1.16768e-01 16 0.435317 2.40539e-02 32 0.826416 3.72579e-03 64 1.66177 7.52979e-05 128 3.26956 1.14699e-07 256 6.37945 5.03033e-13 512 12.6507 8.50134e-16 1024 24.9748 1.18320e-15 2048 50.179 1.77492e-15 4096 100.874 2.76455e-15 8192 202.717 3.45533e-15"}]}