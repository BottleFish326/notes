{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#hello-this-is-indexmd","title":"Hello, this is index.md","text":"<p>\\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\), hello</p> <p>\\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\).</p>"},{"location":"AI/TSFM/Kronos/","title":"Kronos: A Foundation Model for the Language of Financial Markets","text":"<p>Reference Paper</p> <p>Kronos: A Foundation Model for the Language of Financial Markets</p> <p>Author: Department of Automation Tsinghua University</p> <p>Date: 2 Aug 2025</p> <p>Abstract</p> <p>The success of large-scale pre-training paradigm, exemplified by Large Language Models (LLMs), has inspired the development of Time Series Foundation Models (TSFMs). However, their application to financial candlestick (K-line) data remains limited, often underperforming non-pre-trained architectures. Moreover, existing TSFMs often overlook crucial downstream tasks such as volatility prediction and synthetic data generation. To address these limitations, we propose Kronos, a unified, scalable pre-training framework tailored to financial K-line modeling. Kronos introduces a specialized tokenizer that discretizes continuous market information into token sequences, preserving both price dynamics and trade activity patterns. We pre-train Kronos using an autoregressive objective on a massive, multi-market corpus of over 12 billion K-line records from 45 global exchanges, enabling it to learn nuanced temporal and cross-asset representations. Kronos excels in a zero-shot setting across a diverse set of financial tasks. On benchmark datasets, Kronos boosts price series forecasting RankIC by 93% over the leading TSFM and 87% over the best non-pre-trained baseline. It also achieves a 9% lower MAE in volatility forecasting and a 22% improvement in generative fidelity for synthetic K-line sequences. These results establish Kronos as a robust, versatile foundation model for end-to-end financial time series analysis. Our pre-trained model is publicly available at https://github.com/shiyu-coder/Kronos.</p>"},{"location":"AI/TSFM/Kronos/#1-introduction","title":"1. Introduction","text":"<p>Financial markets is a critical and challenging application area for TSFMs, given their inherent data richness, high-frequency observations, and complex, non-stationary temporal dynamics. At the core of this domain are K-line sequences, multivariate time series derived from candle-stick charts that record Open, High, Low, and Close prices, along with trading Volume and Amount(Turnover) over fixed intervals(OHLCVA). These sequences constitute a highly compact, information-dense \"language\" through which market participants interpret prices movements, volatility regimes, liquidity shifts, and colletive sentment.</p>"},{"location":"AI/TSFM/Kronos/#2-motivation","title":"2. Motivation","text":"<p>Despite the promise of TSFMs. Old application to finance faces significant hurdles.</p> <ol> <li>Unique statistical properties of K-line data<ul> <li>Low signal-to-noise ratio, strong non-stationarity, and complex interdependencies among OHLCVA features.</li> <li>These properties are poorly aligned with the inductive biases of general-purpose TSFMs.</li> </ul> </li> <li>Underserved financial domain in pre-training<ul> <li>Most TSFMs are trained on broad, multi-domain corpora where financial data accounts for only a tiny fraction.</li> <li>As a result, financial-specific dynamics are overlooked or averaged out.</li> </ul> </li> <li>Task gaps<ul> <li>Critical finance-specific tasks such as volatility prediction, synthetic data generation, and risk management remain largely unaddressed by existing TSFMs.</li> </ul> </li> </ol> <p>Empirical evidence shows that general-purpose TSFMs often underperform even non-pre-trained, specialized models (like iTransformer) on financial tasks. This reveals a gap: the field lacks a foundation model built specifically for financial time series.</p>"},{"location":"AI/TSFM/Kronos/#3-preliminary","title":"3. Preliminary","text":"<p>Let \\(D\\)-dimensional vector \\(x_t \\in \\mathbb{R}^D\\) denote the K-line observation at discrete time \\(t\\), comprising \\(D\\) key financial indicators. In this work, the dimension was fixed \\(D=6\\) to represent OHLCVA attributes. Given a historical sequence \\(x_{1:T}=(x_1,x_2,...,x_T)\\), our objective is to predict the following \\(H\\) observations \\(\\hat{x}_{T+1:T+H}=(\\hat{x}_{T+1}, \\hat{x}_{T+2},...,\\hat{x}_{T+H})\\).</p> <p>Rather than operating on raw continuous inputs, Kronos first quantizes each multivariate observation \\(x_t\\) into a discrete token \\(b_t\\) via a learnable codebook \\(\\mathcal{C}\\). Consequently, the original sequence \\(x_{1:T}=(x_1,x_2,...,x_T)\\) is mapped to \\(b_{1:T}=(b_1,...,b_T)\\). The forecasting task then reduces to an autoregressive token-sequence modeling problem:</p> \\[ p(\\mathbf{b}_{T+1:T+H}|\\mathbf{b}_{1:T}) = \\sum_{h=1}^{H} p(b_{T+h}|\\mathbf{b}_{1:T+h-1}) \\]"},{"location":"AI/TSFM/Kronos/#4-methodology","title":"4. Methodology","text":"<p>Kronos implements this via a two-phase framework: (1) K-line Tokenization and (2) Autoregressive Pre-training</p>"},{"location":"AI/TSFM/Kronos/#41-k-line-tokenization","title":"4.1 K-line Tokenization","text":"<p>The tokenization is achieved using a Transformer-based autoencoder composed of an encoder \\(E_{\\text{enc}}\\), a quantizer \\(Q\\), and a decoder \\(E_{\\text{dec}}\\).</p> <p>Kronos adapt Binary Spherical Quantization(BSQ) for this task. BSQ quantizes a continuous latent vector \\(\\xi_t\\) into a \\(k\\)-bit binary code \\(b_t \\in \\{-1,1\\}^k\\) by projecting it onto a set of learnable hyperplanes.</p>"},{"location":"AI/TSFM/Kronos/#411-introduction-to-bsq","title":"4.1.1 Introduction to BSQ","text":"<p>Related Paper</p> <p>Image and Video Tokenization with Binary Spherical Quantization</p> <p>Binary Spherical Quantization (BSQ) optimizes over an implicit codebook \\(\\textbf{C}_{BSQ}=\\{-\\frac{1}{\\sqrt{L}}, \\frac{1}{\\sqrt{L}}\\}^L\\), a hypercube projected onto a unit sphere. Each corner \\(\\textbf{c}_k \\in \\textbf{C}_{BSQ}\\) of a hypercube corresponds to a unique token \\(k\\). The quantizer works as follows: it projects some high-dimensional latent embedding \\(\\textbf{z}\\) to a lower-dimensional unit hypersphere \\(\\textbf{u}\\), applies binary quantization per axis \\(\\hat{\\textbf{u}}=sign(\\textbf{u})\\) and back-projects to the quantized vector in the original latent space \\(\\hat{\\textbf{x}}\\). Specifically, we start with an encoded visual input \\(\\textbf{z}=\\mathcal{E}(\\textbf{x}) \\in \\mathbb{R}^d\\). We first linearly project the latent embedding to \\(L\\) dimensions \\(\\textbf{v} = Linear(\\textbf{z}) \\in \\mathbb{R}^L\\), where \\(L \\ll d\\). Next, we obtain project \\(\\textbf{v}\\) onto the unit sphere \\(\\textbf{u}=\\frac{\\textbf{v}}{|\\textbf{v}|}\\) and perform binary quantization to each dimension of \\(u\\) independently \\(\\hat{\\textbf{u}}=\\frac{1}{\\sqrt{L}} sign(\\textbf{\\textbf{u}})\\), where \\(sign(x)\\) is the sign function. To keep outputs on the unit sphere, we map \\(sign(0) \\rightarrow 1\\). We use a Straight-Through Estimator (STE) make the operator differentiable, \\(sign_{STE}(x) = sg(sign(x)\u2212x) + x\\), where \\(sg(\u00b7)\\) denotes the stop-gradient operation. Finally, we back-project the quantized \\(\\hat{\\textbf{u}}\\) to the \\(d\\)-dimensional space \\(\\hat{\\textbf{z}} = Linear(\\hat{\\textbf{u}}) \\in \\mathbb{R}^d\\).</p>"},{"location":"AI/TSFM/Kronos/#412-effectiveness-of-bsq","title":"4.1.2 Effectiveness of BSQ","text":"<p>The effectiveness of BSQ tokenizer can be analyzed from two key perspectivs: its inherent noise suppression and its ability to create a structured, discrete state space suitable for sequence modeling.</p>"},{"location":"AI/TSFM/Kronos/#noise-suppression-and-stability","title":"Noise Suppression And Stability","text":"<p>BSQ's projection of embeddings onto a unit sphere prior to binarization guarantees that the expected distortion is strictly upper-bounded:</p> \\[ \\mathbb{E}_u\\|u-\\hat{u}\\| &lt; \\sqrt{2-\\frac{2}{\\sqrt{L}}} &lt; \\sqrt{2} \\] <p>This bound tightens as the codebook dimension \\(L\\) increases. In contrast, simple methods like sign-based quantization without normalization(e.g., LFQ) lack such a guarantee, leaving them vulnerable to arbitarily large errors from outlier inputs.</p>"},{"location":"AI/TSFM/Kronos/#learning-in-a-compact-and-discrete-state-space","title":"Learning in a Compact and Discrete State Space","text":"<p>High-frequency financial data exists in a high-dimensional, continuous state space. This tokenizer maps these in a finite, discrete vocabulary of tokens. This discretization serves as a powerful form of regularization with two main benefits.</p> <p>Inproved Sample Efficiency and Generalization: Instead of learning a complex function over a continuous space, a downstream model like a Transformer learns to predict transitions and patterns among a finite set of abstract states(tokens). This simplifies the learning tasks and also allows the model to learn robust patterns from fewer examples, which is particularly critical for modeling rare market phenomena.</p> <p>Reduced Overfitting: The quantization process inherently discards fine-grained, potentially noisy variations within each quantization cells. This prevents the model from fitting to spurious artifacts in the tarining data.</p>"},{"location":"AI/TSFM/Kronos/#hyperspherical-geometry-for-tail-sensitivity","title":"Hyperspherical geometry for tail sensitivity","text":"<p>In financial contexts, market returns and price changes often exhibit heavy tails(or fat tails). The heavy-tail distribution of price changes is one of the key sources of trading profits in quantitative investment and cannot be ignored.</p> <p>Unlike standard vector-quantization on the Euclidean sphere, BSQ's binary encoding preserves angular information very efficiently, making it more sensitive to fat-tail data that manifest as sharp directional changes in feature space. This aligns well how microstructure events often appear as abrupt shifts in the \"direction\" of the joint price-volume vector.</p>"},{"location":"AI/TSFM/Kronos/#413-subtoken-factorization","title":"4.1.3 Subtoken Factorization","text":"<p>While a large number of bits \\(k\\) (e.g., \\(k=20\\)) is desirable for capturing rich financial patterns, it results in an exponentially large vocabulary of size \\(2^k\\). To mitigate this, we use the subtoken factorization and factorize the \\(k\\)-bit code into \\(n\\) subspaces. Motivated by the trade-off between parameter savings and latency costs, we set \\(n=2\\). We partition the code into a coarse subtoken \\(b_t^c\\) and a fine subtoken \\(b_t^f\\) of equal bit length. The resulting code \\(b_t\\) is a concatenation of these two subtokens: \\(b_t=[b_t^c,b_t^f]\\).</p> <p>We train the tokenizer with a composite objective that combines a hierachical reconstruction loss and a commitment loss for BSQ:</p> \\[ \\mathcal{L}_{\\text{tokenizer}} = \\mathcal{L}_{\\text{coarse}} + \\mathcal{L}_{\\text{fine}} + \\lambda \\mathcal{L}_{\\text{quant}} \\] <p>where \\(\\lambda\\) is a balancing hyperparameter. The components are defined as:</p> <ul> <li>\\(\\mathcal{L}_{\\text{coarse}}=\\mathbb{E}[\\|\\textbf{x}-E_{\\text{dec}}(\\textbf{b}^c)\\|^2]\\), which trains the coarse subtoken \\(\\textbf{b}^c\\) to form a low-fidelity reconstruction.</li> <li>\\(\\mathcal{L}_{\\text{fine}}=\\mathbb{E}[\\|\\textbf{x}-E_{\\text{dec}}(\\textbf{b})\\|^2]\\), which evaluates the high-fidelity reconstruction using the complete token \\(\\textbf{b}\\).</li> <li>\\(\\mathcal{L}_{\\text{quant}}\\) is the quantization loss from BSQ that regularizes the learning process. It penalizes the L2 distance between continuous latent vector \\(\\xi\\) and their binary codes \\(\\textbf{b}\\), aligning the encoder's outputs with the learned codebook to ensure stable training.</li> </ul>"},{"location":"AI/TSFM/Kronos/#42-hierarchical-autoregressive-modeling","title":"4.2 Hierarchical Autoregressive Modeling","text":"<p>The primary objective is to estimate the joint distribution over the token sequence \\(\\textbf{b}=\\{b_1,...,b_r\\}\\). A simplified form of Equation 1 can be derived as:</p> \\[ p(\\textbf{b}) = \\sum_{t=1}^Tp(b_t|\\textbf{b}_{&lt;t}) \\] <p>where \\(\\textbf{b}_{&lt;t}\\) denotes all preceding tokens up to time \\(t-1\\).</p> <p>Given the hierarchical token design, in which each token is structured as \\(b_t=[b_t^c,b_t^f]\\), we further decompose the conditional probability using the chain rule to explicitly capture the inherent coarse-to-fine dependency:</p> \\[ p(b_t|\\textbf{b}_{&lt;t})=p(b_t^c|\\textbf{b}_{&lt;t}) \\cdot p(b_t^f|\\textbf{b}_{&lt;t}, b_t^c) \\] <p>At time \\(i\\), the subtokens \\(b_i^c\\) and \\(b_i^f\\) are independently projected into vector representations using two distinct embedding layers, resulting in representations \\(e_c(b_i^c)\\) and \\(e_f(b_i^f)\\), respectively. These embeddings are then concatenated and linearly projected to produce a fused input vector:</p> \\[ \\text{v}_i = W_{fuse}([e_c(b_i^c); e_f(b_i^f)]) \\] <p>where \\([\\cdot ; \\cdot]\\) denotes concatenation, and \\(W_{fuse}\\) is a learnable weight matrix responsible for projecting the combined representaion into the model's latent space.</p>"},{"location":"AI/TSFM/Kronos/#coarse-subtoken-prediction","title":"Coarse Subtoken Prediction","text":"<p>The history vector \\(\\textbf{h}_t\\) is projected by a linear head \\(W_c\\) to produce logits for the first subtoken's distribution:</p> \\[ p(b_t^c|\\textbf{b}_{&lt;c})=\\text{softmax}(W_c\\textbf{h}_t) \\]"},{"location":"AI/TSFM/Kronos/#fine-subtoken-prediction","title":"Fine Subtoken Prediction","text":"<p>During the trainiing, we use the model's own prediction from the previous step, \\(\\hat{b}_t^c\\), which is sampled from the predicted distribution \\(p(b_t^c|\\textbf{b}_{&lt;t})\\), rather than using the ground-truth subtoken(i.e., teacher-forcing). We use a cross-attention mechanism where the embedding of \\(\\hat{b}_t^c\\) acts as the query, and the history \\(\\textbf{h}_t\\) provides the key and value. The result is projected by the second head \\(W_f\\):</p> \\[ \\textbf{h}_t^{\\text{update}} = \\text{CrossAttn}(q=e_c(\\hat{b}_t^c), k=v=\\textbf{h}_t) \\] \\[ p(b_t^f|\\textbf{b}_{&lt;t},b_t^c)=\\text{softmax}(W_f\\textbf{h}_t^{\\text{update}}) \\] <p>The overall training objective \\(\\mathcal{L}_{ar}\\) is the negative log-likelihood of the data, summed over both prediction steps:</p> \\[ \\mathcal{L}_{\\text{ar}}=-\\mathbb{E}_{\\textbf{b}\\sim\\mathcal{D}} \\sum_{t=1}^T\\left[\\log p(b_t^c|\\textbf{b}_{&lt;t})+\\log p(b_t^f|\\textbf{b}_{&lt;t}, b_t^c)\\right] \\] <p>where \\(\\mathcal{D}\\) represents the data distribution.</p>"},{"location":"AI/TSFM/Kronos/#5-results-and-conclusion","title":"5. Results and Conclusion","text":""},{"location":"boolean/AdaptiveGridGeneration/","title":"Adaptive grid generation for discretizing impliit complexes","text":"<p>Quote</p> <p>Adaptive grid generation for discretizing implicit complexes</p> <p>date: 19 Jul 2024</p>"},{"location":"boolean/AdaptiveGridGeneration/#1-introduction","title":"1 Introduction","text":"<p>Implicit representations are widely used in computer graphis. A smooth and manifold surface in 3D can be represented as the level set of a scalar function \\(f:\\mathbb{R}^3 \\rightarrow \\mathbb{R}\\), also known as an implicit surface. The implicit surface enjoys a number of benefits, including a simple definition, easy modification, and convenience for operations such as offsets and boolean.</p> <p>To be useful for downstream applications, an implicit surface or complex must be discretized. An ideal grid for discretization should be adaptive in that the finer grid cells are typically where the implicit shape has a non-trival geometry or topology.</p> <p>While adaptive grid generatioin has been extensively studied for polygonizing implicit surfaces, works on implicit complexes have been scarce. A key challenge in the latter is adapting the grid structure to accuartely discretize the intersection of multiple implicit surface.</p>"},{"location":"boolean/AdaptiveGridGeneration/#2-preliminaries","title":"2 Preliminaries","text":""},{"location":"boolean/AdaptiveGridGeneration/#21-level-sets-and-zero-sets","title":"2.1 Level sets and zero sets","text":"<p>The Level set of a scalar function \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) at level \\(s \\in \\mathbb{R}\\) which we denote by \\(f^{-1}(s)\\), is the loci of points where \\(f\\) evaluates to \\(s\\):</p> \\[ f^{-1}(s) = \\{x\\in \\mathbb{R}^n | f(x)=s\\} \\] <p>We call the level set \\(f^{-1}(0)\\) the zero set, which we abbreviate as \\(f^{-1}\\).</p> <p>This definition naturally generalizes to the level set of a vector-valued function \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\), where \\(m &gt; 1\\), as the loci of points where \\(f\\) evaluated to a vectro level \\(s\\). Note that \\(f\\) can be considered as a set of \\(m\\) scalar functions \\(\\{f_1,...,f_m\\}\\), which we called the components of \\(f\\). Geometrically, \\(f^{-1}(s)\\) is the intersection of the \\(m\\) scalar level sets of its components,</p> \\[ f^{-1}(s) = \\bigcap_{i=1}^{m}f_i^{-1}(s_i) \\] <p>where \\(s=\\{s_1,...,s_m\\}\\). As a result, \\(f^{-1}(s)\\) is a \\((n-m)\\)-dimensional manifold, and it generally only exists for \\(m \\le n\\).</p>"},{"location":"boolean/AdaptiveGridGeneration/#3-refinement-criteria-for-zero-set","title":"3. Refinement criteria for zero set","text":"<p>Our criteria answer the following general question: given a function \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) for any dimension \\(n\\) and \\(m \\in [1,n]\\) and an \\(n\\)-simplex \\(t\\), does \\(t\\) need to be refined to better discretize the zero set \\(f^{-1}\\).</p> <p>Sepcial Case</p> <p>In \\(\\mathbb{R}^3\\), our criteria consider a single implicit surface(\\(m=1\\)), the intersection curve of two surfaces(\\(m=2\\)), or the interserction point of three surfaces(\\(m=3\\)).</p> <p>Our criteria follow the same principles as existing ones for implicit surfaces. That is, refinement is not needed if either \\(t\\) does not contain any part of \\(f^{-1}\\), or if \\(f^{-1}\\) is already close enough to the discretization. Specifically, let \\(\\bar{f}\\) be the linear approximation of \\(f\\) inside \\(t\\) by barycentric interpolation of values of \\(f\\) at the vertices of \\(t\\). We consider the zero set of \\(\\bar{f}\\), \\(\\bar{f}^{-1}\\), as the discretization of \\(f^{-1}\\) in \\(t\\). We deem \\(t\\) refinable for \\(f^{-1}\\) if it passes two tests:</p> <ul> <li> <p>Zero-crossing test:</p> \\[ f^{-1} \\cap t \\ne \\emptyset \\] </li> <li> <p>Distance test:</p> \\[ d_H(f^{-1} \\cap t, \\bar{f}^{-1}) &gt; \\epsilon \\] <p>where \\(d_H(X, Y) = \\text{sup}_{x\\in X}d(x,Y)\\) is the one-sided Hausdorff distance from \\(X\\) to \\(Y\\), and \\(\\pesilon\\) is a user-defined threshold.</p> </li> </ul>"},{"location":"boolean/AdaptiveGridGeneration/#31-proxy-construction","title":"3.1 Proxy construction","text":"<p>We consider the calss of functions \\(\\tilde{f}\\) represented as a convex combination with linear precision as our proxy. Such a function is defined by control points \\(p_1,...,p_l\\) that lie inside or on the boundary of the simplex \\(t\\), where each \\(p_i\\) is associated with a control value \\(b^i=\\{b_1^i,...,b_m^i\\}\\). The function, \\(\\tilde{f}\\), is a weighted average of \\(b^i\\),</p> \\[ \\tilde{f}(x)=\\sum_{i=1}^{l}w_i(x)b^i \\] <p>where the weights \\(w_i(x)\\) satisfy, for all \\(x \\in t\\):</p> <ol> <li>Convexity: \\(w_i(x) \\ge 0\\) for \\(i = 1,...,l\\), and \\(\\sum_{i=1}^lw_i(x)=1\\)</li> <li>Linear precision: \\(\\sum_{i=1}^lw_i(x)p_i=x\\)</li> </ol> <p>A example of such \\(\\tilde{f}\\) is the Bezier simplex, where the control points lie on a regular grid in \\(t\\) and the weights \\(w_i\\) are Berstein polynoials. In our implementation, we adopt the cubic Bezier simplex as the proxy. The control points \\(p_i\\) in a cubic Bezier simplex consist of all vertices of \\(t\\), the two trisector on each edge of \\(t\\), and the centroid of each triangle face of \\(t\\).</p>"},{"location":"boolean/AdaptiveGridGeneration/#32-zero-crossing-test","title":"3.2 Zero-crossing test","text":"<p>We now examine the refinement criteria on a proxy function \\(\\tilde{f}\\). The value \\(\\tilde{f}(x)\\) at any point \\(x \\in t\\) lies in the \\(m\\)-dimensional convex hull of \\(b\\). As a result, if there exists some points \\(x\\) such that \\(\\tilde{f}(x)=0\\), then \\(0\\) lies in this convex hull. So a necessary condition to pass the zero-crossing test is</p> \\[ O \\in CH(b) \\] <p>where \\(O\\) is the origin of \\(\\mathbb{R}^m\\) and \\(CH(b)\\) is the convex hull of \\(b\\).</p>"},{"location":"boolean/AdaptiveGridGeneration/#33-distance-test","title":"3.3 Distance test","text":"<p>We next turn to the distance test on the proxy \\(\\tilde{f}\\), which check if</p> \\[ d_H(\\tilde{f}^{-1}\\cap t, \\bar{f}^{-1}) &gt; \\epsilon \\] <p>where \\(\\bar{f}\\) is the linear approximation of \\(f\\) in \\(t\\). We observe that the exact Hausdorff distance has the following upper bound:</p> \\[ \\begin{aligned} d_H(\\tilde{f}^{-1} \\cap t, \\overline{f}^{-1})  &amp;= \\max_{x \\in \\tilde{f}^{-1}\\cap t} d(x, \\overline{f}^{-1}(0)) \\\\ &amp;= \\max_{x \\in \\tilde{f}^{-1}\\cap t} d(x, \\overline{f}^{-1}(\\tilde{f}(x))) \\\\ &amp;\\leq \\max_{x \\in t} d(x, \\overline{f}^{-1}(\\tilde{f}(x))) \\end{aligned} \\] <p>In other words, the upper bound is the maximal distance from any point \\(x \\in t\\) to the level set of the linear function \\(\\bar{f}\\) at the level \\(\\tilde{f}(x)\\).</p> <p>We first consider the simple case of \\(m=1\\). Since \\(\\bar{f}\\) is a scalar function, its level sets are \\((n-1)\\)-dimensional hyperplane in \\(\\mathbb{R}^n\\) orthogonal to the gradient \\(g=\\nabla \\bar{f}\\). The distance from \\(x\\) to the level set of \\(\\bar{f}\\) at level \\(\\tilde{f}(x)\\) is \\(\\frac{\\tilde{f}(x)-\\bar{f}(x)}{|g|}\\) (assuming \\(g\\ne 0\\)).</p> <p>Now consider \\(m &gt; 1\\). The vector level set \\(\\bar{f}^{-1}(\\tilde{f}(x))\\) is the intersection of \\(m\\) scalar level sets \\(\\bar{f}^{-1}_i(\\tilde{f}_i(x))\\), each being a hyperplane orthogonal to a gradient vector \\(g_i=\\nabla \\bar{f}_i\\). Let \\(v\\) be the vector from \\(x\\) to its nearest point on this intersection. We can therefore find \\(v\\) as</p> \\[ v = M(\\tilde{f}(x)-\\bar{f}(x)) \\] <p>where \\(M=g(g^Tg)^{-1}\\)</p> <p>Since linear transformations(such as \\(M\\)) preserve convexity, \\(v\\) lies in this convex hull of the transformed points \\(M(b-\\bar{b})\\). Finally, we arrive at the bound:</p> \\[ d(x, \\bar{f}^{-1}(\\tilde{f}(x))) = |v| = |M(\\tilde{f}(x)-\\bar{f}(x))| \\le \\max_{i=1,...,l}|M(b^i-\\bar{b}^i)| \\] <p>Therefore, a necessary condition to pass the distance test is</p> \\[ \\max_{i=1,...,l}|M(b^i-\\bar{b}^i)| &gt; \\epsilon \\]"},{"location":"boolean/AdaptiveGridGeneration/#4-simplicial-refinement","title":"4. Simplicial refinement","text":"<p>Our implementation is specialized for the practically useful cases of \\(n=2,3\\). Our method is a variation of the calssical longest edge bisection(LEB) method.</p> <p>For any chosen edge \\(e\\), we bisect \\(all\\) cells incident to \\(e\\) at the mid-point of \\(e\\), regardless of whether \\(e\\) is the longest edge of that cell. Specifically, we call an edge refinable if any of its incident cells is refinaable according to the given refinement criteria. At each iteration of the algorithm, we bisect the longest refinable edge in the current grid. We call our method the longest refinable edge bisection(LREB) method.</p> <p>Note</p> <p>This method lack a theoretical bound. But experimental evidence is provided to show that the worst quality among cells enclosing the implicit shape produced by LREB appears to be lower-bound for a variety of implicit surfaces and complexes.</p>"},{"location":"boolean/AdaptiveGridGeneration/#5-result","title":"5. Result","text":""},{"location":"boolean/AdaptiveGridGeneration/#51-zero-crossing-and-distance-test","title":"5.1 Zero-crossing and distance test","text":"<p>We first examine the tests on a scalar zero set whose implicit field is \\(f(x,y)=x^5+x^4-2y^2\\)</p> <p>We then examine the tests on vector zero sets. We start with a 2-component function \\(f=\\{f_1,f_2\\}\\) whose implicit curves are circles with large radius and are almost tangent at their intersection points.</p>"},{"location":"boolean/AdaptiveGridGeneration/#52-performance","title":"5.2 Performance","text":"<p>Observe that the running time is dominated by evaluating the function values and gradients at the grid points(red) and checking the criteria(blue), and the latter in turn is dominated by computing the proxy(solid line) and performing the tests on implicit surfaces(short dashes).</p>"},{"location":"boolean/FastAndRobustMeshArrangement/","title":"Fast and Robust Mesh Arrangements using Floating-point Arithmetic","text":"<p>Quote</p> <p>Fast and Robust Mesh Arrangements using Floating-point Arithmetic</p> <p>date: 27 Nov 2020</p> <p>Abstract</p> <p>We introduce a novel algorithm to transform any generic set of triangles in 3D space into a well-formed simplicial complex. Intersecting elements in the input are correctly identified, subdivided, and connected to arrange a valid configuration, leading to a topologically sound partition of the space into piece-wise linear cells. Our approach does not require the exact coordinates of intersection points to calculate the resulting complex. We represent any intersection point as an unevaluated combination of input vertices. We then extend the recently introduced concept of indirect predicates to define all the necessary geometric tests that, by construction, are both exact and effcient since they fully exploit the floating-point hardware. This design makes our method robust and guaranteed correct, while being virtually as fast as non-robust floating-point based implementations. Compared with existing robust methods, our algorithm o$ers a number of advantages: it is much faster, has a better memory layout, scales well on extremely challenging models, and allows fully exploiting modern multi-core hardware with a parallel implementation. We thoroughly tested our method on thousands of meshes, concluding that it consistently outperforms prior art. We also demonstrate its usefulness in various applications, such as computing effcient mesh booleans, Minkowski sums, and volume meshes.</p>"},{"location":"boolean/FastAndRobustMeshArrangement/#1-problem-statement","title":"1 Problem Statement","text":"<p>Consider a generic set of triangles \\(T\\) with no assumptions, and identify their arrangement in space, that is, a subdivision of the space in cells bounded by the input triangles. To explicitly represent the bounding surface of each cell we subdivide intersecting triangles, thus constructing new points to represent the intersections and connecting them to form the sub-triangles.</p> <p>We achieved this result by first cleaning \\(T\\) from degenerate(null area) elements, and then resolving all the intersections on the remaining triangles, thus producing a modified set \\(T'\\). Therefore, triangles in \\(T'\\) are not only geometrically coincident with \\(T\\) but also from a valid simplicial complex, meaning that they are either disjoint or connected through a shared sub-simplex(i.e., they have an edge or vertex in common).</p> <p>The most critical part of the computation of an arrangement is the representation of points of intersection.</p>"},{"location":"boolean/FastAndRobustMeshArrangement/#2-main-contribution","title":"2 Main Contribution","text":"<p>Main contribution is a novel method to robustly reconstruct the subdivided triangles \\(T'\\) without using the coordinates of intersection points. In the method, indeed, these points are implicitly represented in terms of the input geometic entities that generated them. Their relative positions, the program flow and the connectivity of \\(T'\\) are robustly determined by a novel set of exact geometric predicates.</p>"},{"location":"boolean/FastAndRobustMeshArrangement/#3-representation-and-processing","title":"3 Representation and Processing","text":"<p>Our implicit intersection points keep the memory of the supporting planes and lines of the triangles and edges that generated them, and the vertices of the supporting entities are part of the input, hence exact.</p> <p>Using an explicit representation, this is a relatively simple task to discover the relative position of all the intersection points for existing mesh elements, but for an implicit representation, it's not that obviously. So, we will implement these core functionalities for sets of implicit points, as well as for hybrid sets made by both implicit and explicit points later.</p>"},{"location":"boolean/FastAndRobustMeshArrangement/#31-point-representation","title":"3.1 Point representation","text":"<p>Our algorithm performs computations involving three different types of points, in terms of representation:</p> <ul> <li>explicit(i.e., input) points, for which exact floating-point coordinates are known a priori;</li> <li>point implicitly defined by the intersection of two triangles. We represent these points indirectly using five explicit points</li> <li>point implicitly defined by the intersection of three or more triangles. We represent these points indirectly with three triplets of explicit points, for a total of nine;</li> </ul> <p>Our key to implement these functionalities has been a set of novel geometric predicates that can opeate on any combination of point types.</p> <p>We create a multi-stage filtering to ensure that all the variables are(exact) input values, and the predicate can always return an exact sign:</p> <ol> <li>First, all the calculations are done in floating  point arithmetic. Along with the expression, a semi-static filter is also computed. If the magnitude of the evaluated expression is larger than the filter value, then its sign is guaranteed correct, and the process stops.</li> <li>If not, everything is recomputed using interval arithmetic. If the resulting interval does not contain the zero, the sign is correct and we stop.</li> <li>If not, we recompute everything using floating point expansions which always guarantee correctness.</li> </ol>"},{"location":"boolean/FastAndRobustMeshArrangement/#32-point-orientation","title":"3.2 Point orientation","text":"<p>A very basic geometric test: the computation of the relative position, on a plane, of a point and a line. This operator is called <code>orient2d</code>.</p> <p>Even though our points are embedded in \\(\\mathbb{R}^3\\), determining the orientation of a point with respect to a given line is intrinsically two-dimensional and requires expressing the point coordinates in a 2D frame. The rotatioin would require applying a rotation matrix which may introduce round-off errors. Absolute precision requires to drop a coordinate and therefore do not introduce imprecision in the other two coordinates.</p>"},{"location":"boolean/interactiveMeshBoolean/","title":"Interactive and Robust Mesh Booleans","text":"<p>Reference Paper</p> <p>Interactive and Robust Mesh Booleans</p> <p>Date: 26 May 2022</p> <p>Abstract</p> <p>Boolean operations are among the most used paradigms to create and edit digital shapes. Despite being conceptually simple, the computation of mesh Booleans is notoriously challenging. Main issues come from numerical approximations that make the detection and processing of intersection points inconsistent and unreliable, exposing implementations based on floating point arithmetic to many kinds of degeneracy and failure. Numerical methods based on rational numbers or exact geometric predicates have the needed robustness guarantees, that are achieved at the cost of increased computation times that, as of today, has always restricted the use of robust mesh Booleans to offline applications. We introduce the first algorithm for Boolean operations with robustness guarantees that is capable of operating at interactive frame rates on meshes with up to 200K triangles. We evaluate our tool thoroughly, considering not only interactive applications but also batch processing of large collections of meshes, processing of huge meshes containing millions of elements and variadic Booleans of hundreds of shapes altogether. In all these experiments, we consistently outperform prior art by at least one order of magnitude.</p>"},{"location":"boolean/interactiveMeshBoolean/#introduction","title":"Introduction","text":"<p>In many existing methods, the calculation of a mesh Boolean is framed as a two step process. In the first step, conflicts between mesh elements are resolved, splitting triangles in order to incorporate intersection lines in the connectivity. In the second step, each mesh element is deemed as being inside or outside each input object. The result of a Boolean is eventually computed as a subset of the mesh elements generated in the first step, flitered according to the inside/outside labeling computed in the second step. As shown in the figure below.</p> <p>The major difficulty in implementing a Boolean pipeline comes from the use of finite precision arithmetic, which does not allow to exactly represent and test intersection points.</p>"},{"location":"mathematics/","title":"Mathematics","text":"<p>Here's the section about math, where all the math knowledge needed located.</p>"},{"location":"mathematics/poissonEquation/","title":"Poisson's Equation","text":"<p>Related Works</p> <p>Stanford course Math 220B</p>"},{"location":"mathematics/poissonEquation/#1-problem-description","title":"1. Problem Description","text":"<p>We want to solve the Laplace's equation</p> \\[ \\Delta u = 0 \\] <p>and it's inhomogeneous versions, Poisson's equation</p> \\[ - \\Delta u = f \\] <p>We say a function u satisfying Laplace's equation is a harmonic function.</p>"},{"location":"mathematics/poissonEquation/#2-the-fundamental-solution","title":"2. The Fundamental Solution","text":"<p>Consider Laplace's equation in \\(\\mathbb{R}^n\\),</p> \\[ \\Delta u = 0 \\quad x \\in \\mathbb{R}^n \\] <p>Clearly, there are a lot of functions u which satisfy this equation. In particular, any constant function is harmonic. In addition, any function of the form \\(u(x)=a_1x_1 + ... + a_nx_n\\) for constants \\(a_i\\) is also a solution. Of course, we can list a number of others. Here, however, we are intrested in finding a particular solution of Laplace's equation which will allow us to solve Poisson's equation.</p> <p>Give the symmertic nature of Laplace's equation, we look for a radial solution. That is, we look for a harmonic funtion \\(u\\) on \\(\\mathbb{R}^n\\) such that \\(u(x)=v(|x|)\\). In addition, to being a natural choice due to the symmetry of Laplace's equation, radial solutions are natural to look for because they reduce a PDE to an ODE, which is generally easier to solve. Therefore, we look for a radial solution.</p> <p>If \\(u(x)=v(|x|)\\), then</p> \\[ u_{x_i} = \\frac{x_i}{|x|}v'(|x|) \\quad |x| \\neq 0 \\] <p>which implies</p> \\[ u_{x_ix_i} = \\frac{1}{|x|}v'(|x|) - \\frac{x_i^2}{|x|^3}v'(|x|) + \\frac{x_i^2}{|x|^2}v''(|x|) \\quad |x| \\neq 0 \\] <p>Therefore,</p> \\[ \\Delta u = \\frac{n-1}{|x|}v'(|x|) + v''(|x|) \\] <p>Letting \\(r = |x|\\), we see that \\(u(x)=v(|x|)\\) is a radial solution of Laplace's equation implies \\(v\\) satisfies</p> \\[ \\frac{n-1}{r}v'(r)+v''(r) = 0 \\] <p>And then, from some calculations, we see that for any constants \\(c1\\), \\(c2\\), the function</p> \\[ u(x) \\equiv \\begin{cases} c_1 \\ln |x| + c_2, &amp; \\text{if } n = 2 \\\\ \\frac{c_1}{(2 - n)|x|^{n - 2}} + c_2, &amp; \\text{if } n \\geq 3 \\end{cases} \\tag{3.1} \\] <p>for \\(x \\in \\mathbb{R}^n\\), \\(|x| \\neq 0\\) is a solution of Laplace's equation in \\(\\mathbb{R}^n - {0}\\). We notice that the function \\(u\\) defined in (3.1) satisfies \\(\\Delta u(x)=0\\) for \\(x \\neq 0\\), but at \\(x=0\\), \\(\\Delta u(0)\\) is undefined. We claim that we can choose constants \\(c_1\\) and \\(c_2\\) appropriately so that</p> \\[ - \\Delta_x u = \\delta_0 \\] <p>in the sense of distributions. Recall that \\(\\delta_0\\) is the distribution which is defined as follows. For all \\(\\phi \\in \\mathbb{D}\\),</p> \\[ (\\delta_0, \\phi) = \\phi(0) \\] <p>Claim 1</p> <p>For \\(\\Phi\\) defined in (3.3), \\(\\Phi\\) satisfies</p> \\[ -\\Delta_x\\Phi = \\delta_0 \\] <p>in the sense of distributions. That is, for all \\(g\\in \\mathcal{D}\\),</p> \\[ -\\int_{\\mathbb{R}^n}\\Phi(x)\\Delta_xg(x)\\,dx=g(0) \\] Proof <p>The proof of this claim is a little complex. You can reference to this handouts: Stanford course Math 220B laplace</p> <p>For now, though, assume we can prove this. That is, assume we can find constants \\(c_1\\),\\(c_2\\) such that \\(u\\) defined in (3.1) satisfied</p> \\[ - \\Delta_x u = \\delta_0 \\tag{3.2} \\] <p>Let \\(\\Phi\\) denote the solution of (3.2). Then, define</p> \\[ v(x) = \\int_{\\mathbb{R}^n} \\Phi(x - y) f(y) \\, dy \\] <p>\\(Formally\\), we compute the Laplacian of \\(v\\) as follows,</p> \\[ \\begin{aligned} - \\Delta_x v &amp;= - \\int_{\\mathbb{R}^n} \\Delta_x \\Phi(x - y) f(y) \\, dy \\\\              &amp;= - \\int_{\\mathbb{R}^n} \\Delta_y \\Phi(x - y) f(y) \\, dy \\\\              &amp;= \\int_{\\mathbb{R}^n} \\delta_x f(y) \\, dy = f(x) \\end{aligned} \\] <p>That is, \\(v\\) is a solution of Poisson's equation! Of course, this set of equalities above is entirely formal. We have not prove anything yet. However, we have motivated a solution formula for Poisson's equation from a solution to (3.2). We now return to using the radial solution (3.1) to find a solution of (3.2).</p> <p>Define the function \\(\\Phi\\) as follows. For \\(|x| \\neq 0\\), let</p> \\[ \\Phi(x) =  \\begin{cases} -\\dfrac{1}{2\\pi} \\ln |x|, \\quad \\text{if } n = 2\\\\ \\dfrac{1}{n(n-2)\\alpha(n)} \\cdot \\dfrac{1}{|x|^{n-2}}, \\quad \\text{if } n \\geq 3 \\end{cases} \\tag{3.3} \\] <p>where \\(\\alpha (n)\\) is the volume of the unit ball in \\(\\mathbb{R}^n\\). We see that \\(\\Phi\\) satisfies Laplace's equation on \\(\\mathbb{R}^n-{0}\\). As we will show in the following claim, \\(\\Phi\\) satisfies \\(-\\Delta_x \\Phi = \\delta_0\\). For this reason, we call \\(\\Phi\\) the fundamental solution of Laplace's equation.</p>"},{"location":"mathematics/poissonEquation/#3-solving-poissons-equation","title":"3. Solving Poisson's Equation","text":"<p>We now return to solving Poisson's Equation.</p> \\[ - \\Delta u = f, \\quad x \\in \\mathbb{R}^n \\] <p>From our discussion before, we expect the function</p> \\[ v(x) \\equiv \\int_{\\mathbb{R}^n} \\Phi(x-y) f(y)\\, dy \\] <p>to give us a solution of Poisson's Equation. We now prove that this is in fact true. First, we make a remark.</p> <p>Remark. If we hope that the function \\(v\\) defined above solves Poisson's equation, we must first verify that integral actually converges. If we assume \\(f\\) has compact support on some bounded set \\(K\\) in \\(\\mathbb{R}^n\\), then we see that</p> \\[ \\int_{\\mathbb{R}^n} \\Phi(x-y)f(y) \\, dy \\leq \\|f\\|_{L^\\infty} \\int_{K} \\big| \\Phi(x-y) \\big| \\, dy \\] <p>If we additionally assume that \\(f\\) is bounded, then \\(\\|f\\|_{L^\\infty} \\leq C\\). It is left as an exercise to verify that</p> \\[ \\int_{K} \\big| \\Phi(x-y) \\big| \\, dy \\le + \\infty \\] <p>on any compact set \\(K\\).</p> <p>Theorem 2.</p> <p>Assume \\(f \\in C^2(\\mathbb{R}^n)\\) and has compact support. Let</p> \\[ u(x) \\equiv \\int_{\\mathbb{R}^n} \\Phi(x-y)f(y) \\, dy \\] <p>where \\(\\Phi\\) is the fundamental solution of Laplace's equation. Then</p> <ol> <li>\\(u \\in C^2(\\mathbb{R}^n)\\)</li> <li>\\(- \\Delta u = f \\quad in \\quad \\mathbb{R}^n\\)</li> </ol> Proof <ol> <li>By a change of variables, we write</li> </ol> \\[ u(x)=\\int_{\\mathbb{R}^n}\\Phi(x-y)f(y)\\,dy = \\int_{\\mathbb{R}^n}\\Phi(y)f(x-y)\\,dy \\] <p>Let \\(e_i=(...,0,1,0,...)\\) be the unit vector in \\(\\mathbb{R}^n\\) with a \\(1\\) in the \\(i^{th}\\) slot. Then</p> \\[ \\frac{u(x+he_i)-u(x)}{h} = \\int_{\\mathbb{R}^n}\\Phi(y)\\left[\\frac{f(x+he_i-y)-f(x-y)}{h}\\right]\\,dy \\] <p>Now \\(f\\inC^2\\) implies</p> \\[ \\frac{f(x+he_i-y)-f(x-y)}{h} \\rightarrow \\frac{\\partial f}{\\partial x_i}(x-y) as h \\rightarrow 0 \\] <p>uniformly on \\(\\mathbb{R}^n\\). Therefore,</p> \\[ \\frac{\\partial u}{\\partial x_i}(x) = \\int_{\\mathbb{R}^n}\\Phi(y)\\frac{\\partial f}{\\partial x_i}(x-y)\\,dy \\] <p>Similarly,</p> \\[ \\frac{\\partial^2 u}{\\partial x_i\\partial x_j}(x) = \\int_{\\mathbb{R}^n}\\Phi(y)\\frac{\\partial^2 f}{\\partial x_i\\partial x_j}(x-y)\\,dy \\] <p>This function is continuous because the right-hand side is continuous.</p> <ol> <li>By the above calculations and Claim \\(1\\), we see that</li> </ol> \\[ \\begin{aligned} \\Delta_xu(x)&amp;=\\int_{\\mathbb{R}^n}\\Phi(y)\\Delta_xf(x-y)\\,dy \\\\ &amp;=\\int_{\\mathbb{R}^n}\\Phi(y)\\Delta_yf(x-y)\\,dy \\\\ &amp;=-f(x) \\end{aligned} \\]"},{"location":"mathematics/solidAngleEllipse/","title":"Solid Angle of Ellipse","text":"<p>Related Papers</p> <p>Analytical solution for the solid angle subtended at any point by an ellipse via a point source radiation vector potential</p>"},{"location":"mathematics/solidAngleEllipse/#1-problem-description","title":"1. Problem Description","text":"<p>Calculate the (signed) solid angle for an ellipse from the origin \\((0,0,0)\\)</p> <p>In short, complete the function s.t. <pre><code>/**\n * @brief Computes the solid angle for an ellipse from the origin(0,0,0)\n *\n * @param c Center of the ellipse. R^3\n * @param n Normal vector of the ellipse. R^3\n * @param d Direction of the long axis. R^3  ensure that &lt;n, d&gt; = 0\n * @param a Semi-major axis length of the ellipse. R\n * @param b Semi-minor axis length of the ellipse. R ensure that a &gt;= b &gt;= 0\n * @return The solid angle in steradians.\n */\ndouble solid_angle_ellipse(\n    const Eigen::Vector3d &amp;c,\n    const Eigen::Vector3d &amp;n,\n    const Eigen::Vector3d &amp;d,\n    const double a,\n    const double b\n);\n</code></pre></p>"},{"location":"mathematics/solidAngleEllipse/#2-math-expression-needed","title":"2. Math Expression Needed","text":""},{"location":"mathematics/solidAngleEllipse/#21-problem-simplify","title":"2.1 Problem Simplify","text":"<p>We can assume that the Normal vector of the ellipse \\(\\boldsymbol{n}\\) is parallel to the z-axis and the direction of the long axis \\(\\boldsymbol{d}\\) is parallel to the x-axis, because it's easy to apply a coordinate transformation to satisfy this condition.</p> <p>So, in the following expression, we assume that \\(\\boldsymbol{n} = (0,0,1)\\) and \\(\\boldsymbol{d} = (1,0,0)\\)</p> <p>Also, we assume that \\(\\boldsymbol{c} = (p,q,h)\\)</p>"},{"location":"mathematics/solidAngleEllipse/#22-analytical-solution-for-the-coaxial-case","title":"2.2 Analytical solution for the coaxial case","text":"<p>It's easy to calculate the solid angle using the eletric field. As we suppose a point charge is placed at the origin, and the soliet angle problem can be transformed into a calculation of the electric flux.</p> <p>So the answer is</p> \\[ \\Omega(a, b, h) = 2\\pi \\left(1 - \\Lambda_0(\\beta, k)\\right) \\tag{57} \\] <p>where</p> \\[ \\beta = \\arcsin \\sqrt{ \\frac{h^2}{h^2 + b^2} } \\tag{58} \\] \\[ k = \\sqrt{\\frac{a^2-b^2}{h^2+a^2}} \\] <p>and</p> \\[ \\Lambda_0(\\beta,k) = \\frac{2}{\\pi}[\\textbf{E}(k)F(\\beta,k')+\\textbf{K}(k)E(\\beta,k)-\\textbf{K}(k)F(\\beta,k)] \\] <p>where \\(k' \\equiv \\sqrt{1-k^2}\\)</p>"},{"location":"mathematics/solidAngleEllipse/#23-analytical-case-related-to-sections-of-a-right-circular-cone","title":"2.3 Analytical case related to sections of a right circular cone","text":"<p>We can apply a coordiante transformation such that, in the transformerd frame, the \\(z'\\)-axis is aligned with the direction from the ellipse center to the observation point, and the \\(x'\\) axis is aligned with the ellipse's major axis.</p> <p>After this transformation, the resulting elliptical cone can be characterized by its new effective semi-axes, which can then be used to compute the solid angle.</p> <p>And here's the new axis length after transformation.</p> \\[ \\bar{a}^2 = a^2-h^2-p^2 + \\sqrt{(p^2-a^2)^2+2h^2(p^2+a^2)+h^4} \\] \\[ \\bar{b}^2 = 2b^2 \\] \\[ \\bar{h}^2 = h^2+p^2-a^2 + \\sqrt{(p^2-a^2)^2+2h^2(p^2+a^2)+h^4} \\] <p>and then, we can apply the conclusion of Chapter 2.2 to calculate the answer, but here's the point, in this occasion, we're not sure that if \\(a &gt; b\\) or \\(a &lt; b\\), so, we should first swap \\(a\\) and \\(b\\) if \\(a &lt; b\\) before apply the conclusion.</p>"},{"location":"mathematics/solidAngleEllipse/#24-general-solution","title":"2.4 General Solution","text":""},{"location":"mathematics/solidAngleEllipse/#241-rotation-of-the-coordiante-system","title":"2.4.1 Rotation of the coordiante system","text":"<p>In the \\((x,y,z)\\) coordinate system, the sheared core obeys the equation</p> \\[     \\left(\\frac{x}{a} - \\frac{pz}{ah}\\right)^2 +     \\left(\\frac{y}{b} - \\frac{qz}{bh}\\right)^2 -     \\left(\\frac{z}{h}\\right) = 0     \\tag{117} \\] <p>we need to rotate the coordinate system \\((x,y,z)\\) to \\((x',y',z')\\) where the \\(z'\\) axis lies along the center line of the right elliptic cone.</p> <p>This rotation need a right-handed rotation by an angle \\(\\gamma\\) about the \\(y\\)-axis to give a new coordinate system \\((\\hat{x},\\hat{y},\\hat{z})\\) and a additional right-handed rotation by an angle \\(\\psi\\) about the \\(\\hat{x}\\) axis to give the new coordinate system \\((x',y',z')\\)</p> <p>Then, we get</p> \\[ \\begin{bmatrix}   x'\\\\   y'\\\\   z' \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\psi &amp; \\sin\\psi \\\\ 0 &amp; -\\sin\\psi &amp; \\cos\\psi \\end{bmatrix} \\begin{bmatrix} \\cos\\gamma &amp; 0 &amp; -\\sin\\gamma \\\\ 0 &amp; 1 &amp; 0 \\\\ \\sin\\gamma &amp; 0 &amp; \\cos\\gamma \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\tag{119} \\] <p>Substituting Eq.(119) into Eq.(117), expanding and collecting terms gives</p> \\[ A_1x'^2 + B_1x'y' + C_1y'^2+D_1x'+E_1y'=F_1 \\tag{121} \\] <p>where</p> \\[ A_1 = \\frac{1}{a^2} + A \\sin^2\\gamma + B \\sin 2\\gamma \\] \\[ B_1 = 2C \\sin\\gamma \\cos\\psi - 2B \\cos 2\\gamma \\sin\\psi - A \\sin 2\\gamma \\sin\\psi \\] \\[ C_1 = \\frac{\\sin^2 \\psi}{a^2} + \\frac{\\cos^2 \\psi}{b^2} - B \\sin 2\\gamma \\sin^2 \\psi - C \\cos\\gamma \\sin 2\\psi + A \\cos^2\\gamma \\sin^2 \\psi \\] \\[ D_1 = -(A \\sin 2\\gamma \\cos\\psi + 2B \\cos 2\\gamma \\cos\\psi + 2C \\sin\\gamma \\sin\\psi) z' \\] \\[ E_1 = \\left(D \\sin 2\\psi - B \\sin 2\\gamma \\sin 2\\psi + A \\cos^2 \\gamma \\sin 2\\psi - 2C \\cos\\gamma \\cos 2\\psi \\right) z' \\] \\[ F_1 = \\left( B \\sin 2\\gamma \\cos^2 \\psi - \\frac{\\cos^2 \\psi}{a^2} - \\frac{\\sin^2 \\psi}{b^2} - A \\cos^2 \\gamma \\cos^2 \\psi - C \\cos\\gamma \\sin 2\\psi \\right) z'^2 \\] <p>and where</p> \\[ A = \\frac{p^2}{a^2 h^2} + \\frac{q^2}{b^2 h^2} - \\frac{1}{a^2} - \\frac{1}{h^2} \\] \\[ B = \\frac{p h}{a^2 h^2} \\] \\[ C = \\frac{q h}{b^2 h^2} \\] \\[ D = \\frac{1}{a^2} - \\frac{1}{b^2} \\]"},{"location":"mathematics/solidAngleEllipse/#242-solution-for-the-rotation-angle","title":"2.4.2  Solution for the rotation angle","text":"<p>For the \\(z'\\) axis to lie along the axis of the cone, the coefficients \\(D_1\\) and \\(E_1\\) must vanish simultaneously and hence</p> \\[ A \\sin 2\\gamma + 2B \\cos 2\\gamma + 2C \\sin\\gamma \\tan\\psi = 0 \\] \\[ \\left(D - B \\sin 2\\gamma + A \\cos^2 \\gamma \\right) \\tan 2\\psi - 2C \\cos\\gamma = 0 \\] <p>After solving the system of equations, we get</p> \\[ \\gamma_j = \\arctan(w_j) \\] \\[ \\psi_j = -\\arctan\\left( \\frac{A \\sin 2\\gamma_j + 2B \\cos 2\\gamma_j}{2C \\sin \\gamma_j} \\right) \\] <p>where \\(w_j\\), \\(j \\in {1,2,3}\\) is the three roots of the function</p> \\[ w^3 - \\left( \\frac{A}{B} + \\frac{B}{D} + \\frac{C^2}{BD} \\right) w^2 + \\left( \\frac{A}{D} - 1 \\right) w + \\frac{B}{D} = 0 \\] <p>it is proved from the geometry that all three roots are real number.</p> <p>only one root that satisfied \\(B_1^2-4A_1C_1 &lt; 0\\) is the correct root that needed.</p>"},{"location":"mathematics/solidAngleEllipse/#243-equation-of-the-ellipse-in-canonical-form","title":"2.4.3 Equation of the ellipse in canonical form","text":"<p>The value of \\(z'\\) in Eq.(127) for \\(F_1\\) is arbitrary and can be conveniently set to unity.</p> <p>Since \\(E_1\\) and \\(F_1\\) is zero now, we can apply another right-handed rotation by an angle \\(\\lambda\\) about the \\(z'\\) axis to set \\(B_1\\) to zero.</p> <p>We denoting the new rotated coordinates by \\((\\bar{x},\\bar{y},\\bar{z})\\), then do the same steps of the previous part.</p> <p>Then, we can get the final equation of the ellipse in the form</p> \\[ A_2\\bar{x}^2+C_2\\bar{y}^2=F_1 \\] <p>where</p> \\[ A_2 = \\frac{A_1 + C_1 + \\operatorname{sgn}(A_1 - C_1) \\sqrt{B_1^2 + (A_1 - C_1)^2}}{2} \\] <p>and</p> \\[ C_2 = \\frac{A_1 + C_1 - \\operatorname{sgn}(A_1 - C_1) \\sqrt{B_1^2 + (A_1 - C_1)^2}}{2} \\] <p>as the equation of Eq.(57), we can get the final solid angle which is equal to </p> \\[ \\Omega \\left( \\sqrt{\\frac{F_1}{A_2}}, \\sqrt{\\frac{F_1}{C_2}} \\right) \\]"},{"location":"mathematics/solidAngleEllipse/#3-results","title":"3. Results","text":""},{"location":"mathematics/solidAngleEllipse/#31-time-complexity","title":"3.1 time Complexity","text":"<p>The horizontal axis repersents the number of triangles used to divide the ellipse in the numerical intergration, while the vertical axis shows the average time required to compute the solid angle. The orange curve correspoinds to the numerical-integration approach, whereas the red dashed line denotes the analytical formula(\\(1.9123\\mu s\\)).</p>"},{"location":"mathematics/solidAngleEllipse/#32-computational-error","title":"3.2 computational error","text":"<p>The error was determined by comparing our results with those reported in the paper, which are accurate to 20 significant digits The horizontal axis repersents the number of triangles used to divide the ellipse in the numerical intergration, while the vertical axis shows the average error in the calculated solid angle. The orange curve correspoinds to the numerical-integration approach, whereas the red dahed line denotes the analytical formula(\\(9.20974e-15\\)).</p> <p>summery table</p> N Time (\u03bcs) Error 1 0.0798565 8.52715e-01 2 0.101816 6.97061e-01 4 0.144651 4.03314e-01 8 0.247237 9.64311e-02 16 0.426442 2.40481e-02 32 0.814843 3.64688e-03 64 1.57491 7.55273e-05 analytical fomula 1.9123 9.20974e-15 128 3.22987 1.14700e-07 256 6.43509 5.02762e-13 512 13.982 6.69005e-16 1024 24.3732 1.48253e-15 2048 48.5727 8.52766e-16 4096 107.324 2.98612e-15 8192 196.24 3.20553e-15"},{"location":"mathematics/solidAngleEllipse/#33-accelerate","title":"3.3 accelerate","text":"<p>if we disable the elliptic integral promotion to long double, we can get the table following</p> n Time (\u03bcs) Error 1 0.0564791 8.22507e-01 2 0.0774804 1.13996e+00 4 0.126105 3.63865e-01 8 0.223437 1.16768e-01 analytical fomula 0.314463 9.25467e-15 16 0.413136 2.40539e-02 32 0.763126 3.72579e-03 64 1.57758 7.52979e-05 128 3.1183 1.14699e-07 256 6.22602 5.03033e-13 512 13.5295 8.50134e-16 1024 24.4535 1.18320e-15 2048 49.7387 1.77492e-15 4096 127.653 2.76455e-15 8192 209.726 3.45533e-15 <p>if we disable the elliptic integral promotion to double, we can get the table following. But for some extreme samples, analytical fomula error would goto 1e-5, which will maybe not be accpeted.</p> n Time (\u03bcs) Error 1 0.050011 8.22507e-01 2 0.0698501 1.13996e+00 4 0.134739 3.63865e-01 analytical fomula 0.186762 5.19641e-07 8 0.230931 1.16768e-01 16 0.435317 2.40539e-02 32 0.826416 3.72579e-03 64 1.66177 7.52979e-05 128 3.26956 1.14699e-07 256 6.37945 5.03033e-13 512 12.6507 8.50134e-16 1024 24.9748 1.18320e-15 2048 50.179 1.77492e-15 4096 100.874 2.76455e-15 8192 202.717 3.45533e-15"}]}